{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aravind Patnam\n",
    "Jeremy Tan\n",
    "Timothy Le\n",
    "\n",
    "CSE-184 Final Project - Trending Here Trending There \n",
    "An analysis on trending and nontrending Youtube videos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## imports\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import requests\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from nltk.book import *\n",
    "from nltk.corpus import stopwords\n",
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "import random\n",
    "from nltk import classify\n",
    "from nltk import NaiveBayesClassifier\n",
    "from nltk.tokenize import word_tokenize\n",
    "import seaborn as sns\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "sns.set_context('notebook')\n",
    "import warnings\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "from nltk.corpus import stopwords\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import re\n",
    "import random\n",
    "from gensim import corpora\n",
    "import gensim\n",
    "import pickle\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import spacy\n",
    "spacy.load('en')\n",
    "from spacy.lang.en import English\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet as wn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os import path\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "import bokeh.plotting as bp\n",
    "from bokeh.models import HoverTool, ColorBar, NumeralTickFormatter, LinearColorMapper, LassoSelectTool, ResetTool, PanTool, BoxSelectTool, TapTool, PolySelectTool\n",
    "from bokeh.palettes import plasma\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.transform import transform\n",
    "import bokeh.io\n",
    "import bokeh.plotting as bpl\n",
    "import bokeh.models as bmo\n",
    "from bokeh.palettes import d3\n",
    "from bokeh.transform import linear_cmap\n",
    "from bokeh.models import SingleIntervalTicker, LinearAxis\n",
    "from bokeh.layouts import gridplot\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from bokeh.io import show, output_file\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.palettes import Category20\n",
    "from bokeh.models import Legend\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.io import output_file\n",
    "from bokeh.models import ColumnDataSource, CDSView, GroupFilter\n",
    "import bokeh\n",
    "from bokeh.palettes import Category20c, Plasma\n",
    "from bokeh.models import HoverTool\n",
    "from bokeh.io import show\n",
    "from bokeh.models import ColumnDataSource, FactorRange\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.palettes import Spectral6\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.transform import factor_cmap\n",
    "from bokeh.models import HoverTool, ColorBar, NumeralTickFormatter, LinearColorMapper, LassoSelectTool, ResetTool, PanTool, BoxSelectTool, TapTool, PolySelectTool\n",
    "# Bokeh Library\n",
    "from bokeh.io import output_file\n",
    "from bokeh.models.widgets import Tabs, Panel\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## written by Aravind Patnam and Jeremy Tan\n",
    "#### get all csv dataframes for trending. These files should be in the same directory. \n",
    "US_trending_df = pd.read_csv('data/USvideos.csv') #USA\n",
    "CA_trending_df = pd.read_csv('data/CAvideos.csv') #CANADA\n",
    "DE_trending_df = pd.read_csv('data/DEvideos.csv') #GERMANY\n",
    "FR_trending_df = pd.read_csv('data/FRvideos.csv') #FRANCE\n",
    "GB_trending_df = pd.read_csv('data/GBvideos.csv') #GREAT BRITAIN\n",
    "IN_trending_df = pd.read_csv('data/INvideos.csv') #INDIA\n",
    "JP_trending_df = pd.read_csv('data/JPvideos.csv', encoding='ISO-8859-1') #JAPAN\n",
    "KR_trending_df = pd.read_csv('data/KRvideos.csv' , encoding='ISO-8859-1') #SOUTH KOREA\n",
    "MX_trending_df = pd.read_csv('data/MXvideos.csv', encoding='ISO-8859-1') #MEXICO\n",
    "RU_trending_df = pd.read_csv('data/RUvideos.csv', encoding='ISO-8859-1') #RUSSIA\n",
    "\n",
    "list_of_all_trending_dfs = [US_trending_df, CA_trending_df, DE_trending_df, FR_trending_df, GB_trending_df, IN_trending_df,\n",
    "                           JP_trending_df, KR_trending_df, MX_trending_df, RU_trending_df]\n",
    "list_of_csvs = ['data/USvideos.csv','data/CAvideos.csv', 'data/DEvideos.csv', 'data/FRvideos.csv', 'data/GBvideos.csv', 'data/INvideos.csv', 'data/JPvideos.csv','data/KRvideos.csv', 'data/MXvideos.csv', 'data/RUvideos.csv' ]\n",
    "\n",
    "big_df = list()\n",
    "for csv in list_of_csvs:\n",
    "    # use encoding to bypass utf error\n",
    "    df = pd.read_csv(csv, index_col='video_id', encoding='ISO-8859-1')\n",
    "    # add new column called \"country\" to indentify which videos the csv are coming from\n",
    "    # depending on your path name, this will break as it looks at the path name \n",
    "    df['country'] = csv[5:7]\n",
    "    big_df.append(df)\n",
    "    \n",
    "full_trending_df = pd.concat(big_df)\n",
    "full_trending_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### reformatting and detecting nans\n",
    "\n",
    "# reformat trending_date\n",
    "full_trending_df['trending_date'] = pd.to_datetime(full_trending_df['trending_date'],errors='coerce', format='%y.%d.%m')\n",
    "full_trending_df['publish_time'] = pd.to_datetime(full_trending_df['publish_time'], errors='coerce', format='%Y-%m-%dT%H:%M:%S.%fZ')\n",
    "\n",
    "# detects any nans \n",
    "full_trending_df = full_trending_df[full_trending_df['trending_date'].notnull()]\n",
    "full_trending_df = full_trending_df[full_trending_df['publish_time'].notnull()]\n",
    "\n",
    "# drop all nans byrmeoving them \n",
    "full_trending_df = full_trending_df.dropna(how='any',inplace=False, axis = 0)\n",
    "\n",
    "# this is done already so don't run it twice\n",
    "full_trending_df.insert(4, 'publish_date', full_trending_df['publish_time'].dt.date)\n",
    "full_trending_df['publish_time'] = full_trending_df['publish_time'].dt.time\n",
    "\n",
    "# set index by video id and sort by trending dates \n",
    "full_trending_df_fill = full_trending_df.reset_index().sort_values('trending_date').set_index('video_id')\n",
    "# set index by vide id and sort by trending dates, but make sure to drop duplicates\n",
    "full_trending_df = full_trending_df.reset_index().sort_values('trending_date').drop_duplicates('video_id',keep='last').set_index('video_id')\n",
    "# videos[['publish_date','publish_time']].head()\n",
    "full_trending_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## written by Aravind Patnam\n",
    "\n",
    "## run these to get the non trending datasets generated from the youtube api. These files should be in the same directory\n",
    "not_trending_us_df = pd.read_csv('data/not_trending_us_df.csv')\n",
    "not_trending_ca_df = pd.read_csv('data/not_trending_ca_df.csv')\n",
    "not_trending_de_df = pd.read_csv('data/not_trending_de_df.csv')\n",
    "not_trending_fr_df = pd.read_csv('data/not_trending_fr_df.csv')\n",
    "not_trending_gb_df = pd.read_csv('data/not_trending_gb_df.csv')\n",
    "not_trending_in_df = pd.read_csv('data/not_trending_in_df.csv')\n",
    "not_trending_jp_df = pd.read_csv('data/not_trending_jp_df.csv')\n",
    "not_trending_kr_df = pd.read_csv('data/not_trending_kr_df.csv')\n",
    "not_trending_mx_df = pd.read_csv('data/not_trending_mx_df.csv')\n",
    "not_trending_ru_df = pd.read_csv('data/not_trending_ru_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below cell was used to fetch nontrending videos using the youtube API. For each nontrending video, a related non-trending video was fetched. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## written by Aravind Patnam\n",
    "\n",
    "### do not run this unless you have about 1 hour and 10 Youtube API keys and a lot of time!!! Use above already generated datasets for testing. \n",
    "f = open(\"apiKey\", \"r\")\n",
    "key = f.read()\n",
    "## for each videoId, find a related video\n",
    "def do_search_youtube_request(videoId):\n",
    "    url = \"https://www.googleapis.com/youtube/v3/search?part=snippet&maxResults=50&order=relevance&relatedToVideoId={}&type=video&videoDefinition=any&key={}\".format(videoId, key)\n",
    "    r = requests.get(url)\n",
    "    return r\n",
    "\n",
    "## given a set of videoIds, find insights (statistics, tags, etc)\n",
    "def find_video_insights(videoIds):\n",
    "    print(videoIds)\n",
    "    url = 'https://www.googleapis.com/youtube/v3/videos?part=snippet%2CcontentDetails%2Cstatistics&id={}&key={}'.format(videoIds, key)\n",
    "    r = requests.get(url)\n",
    "    return r\n",
    "\n",
    "## call this with 1 country at a time \n",
    "def process_youtube_requests(videoIds):\n",
    "    df = pd.DataFrame(columns=['video_id', 'title', 'channel_title', 'category_id', 'publish_time', 'tags', \n",
    "                               'views', 'likes', 'dislikes', 'comment_count', 'description'])\n",
    "    relatedVideoIds = []\n",
    "    for videoId in videoIds:\n",
    "        try: \n",
    "            response = do_search_youtube_request(videoId)\n",
    "            time.sleep(2)\n",
    "            if (response.status_code == 200):\n",
    "                r1 = response.json()\n",
    "                relatedVideoIdItems = r1['items']\n",
    "                for id in relatedVideoIdItems:\n",
    "                    relatedVideoId = id['id']['videoId']\n",
    "                    relatedVideoIds.append(str(relatedVideoId))\n",
    "            else:\n",
    "                print(response.status_code)\n",
    "        except:\n",
    "            print (\"Something went wrong here! 2\")\n",
    "    random.shuffle(relatedVideoIds)\n",
    "    videoIdsForInsights = []\n",
    "    for i in range(0, len(relatedVideoIds), 50):\n",
    "        videoIdsForInsights.append(relatedVideoIds[i:i + n])\n",
    "    for videoIdList in videoIdsForInsights:\n",
    "        videoIdsStr = '%2C'.join([str(elem) for elem in videoIdList])\n",
    "        r2 = find_video_insights(videoIdsStr)\n",
    "        time.sleep(2)\n",
    "        if (r2.status_code == 200):\n",
    "            r = r2.json()\n",
    "            i = 0\n",
    "            while (i < len(videoIdList)):\n",
    "                try:\n",
    "                    id = videoIdList[i]\n",
    "                    title = (r['items'][i]['snippet']['title'])\n",
    "                    channel_title = (r['items'][i]['snippet']['channelTitle'])\n",
    "                    category_id = (r['items'][i]['snippet']['categoryId'])\n",
    "                    publish_time = (r['items'][i]['snippet']['publishedAt'])\n",
    "                    tags = '|'.join((r['items'][i]['snippet']['tags']))\n",
    "                    views = (r['items'][i]['statistics']['viewCount'])\n",
    "                    likes = (r['items'][i]['statistics']['likeCount'])\n",
    "                    dislikes = (r['items'][i]['statistics']['dislikeCount'])\n",
    "                    comment_count = (r['items'][i]['statistics']['commentCount'])\n",
    "                    description = (r['items'][i]['snippet']['description'])\n",
    "                    data = {'video_id': id, 'title': title, 'channel_title': channel_title, 'category_id' : category_id,\n",
    "                           'publish_time' : publish_time, 'tags' : tags, 'views' : views, 'likes' : likes, 'dislikes' : dislikes,\n",
    "                           'comment_count' : comment_count, 'description' : description}\n",
    "                    df = df.append(data, ignore_index = True)\n",
    "                except:\n",
    "                    print(\"Something went wrong! 3\")\n",
    "                i = i + 1\n",
    "        else:\n",
    "            print(\"Something went wrong! 4\")\n",
    "            print (r2.status_code)\n",
    "            print(r2.text)\n",
    "    return df     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## written by Aravind Patnam\n",
    "\n",
    "### do not run this unless you have a lot of time and 10 Youtube API keys!!!. Datasets are already generated for you for testing. \n",
    "n = 50\n",
    "US_trending_videoIds = US_trending_df.sample(n)['video_id'].tolist()\n",
    "CA_trending_videoIds = CA_trending_df.sample(n)['video_id'].tolist()\n",
    "DE_trending_videoIds = DE_trending_df.sample(n)['video_id'].tolist()\n",
    "FR_trending_videoIds = FR_trending_df.sample(n)['video_id'].tolist()\n",
    "GB_trending_videoIds = GB_trending_df.sample(n)['video_id'].tolist()\n",
    "IN_trending_videoIds = IN_trending_df.sample(n)['video_id'].tolist()\n",
    "JP_trending_videoIds = JP_trending_df.sample(n)['video_id'].tolist()\n",
    "KR_trending_videoIds = KR_trending_df.sample(n)['video_id'].tolist()\n",
    "MX_trending_videoIds = MX_trending_df.sample(n)['video_id'].tolist()\n",
    "RU_trending_videoIds = RU_trending_df.sample(n)['video_id'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## written by Aravind Patnam\n",
    "### do not run this unless you have a lot of time and 10 Youtube API keys!!! Datasets are already generated for you for testing. \n",
    "## do following requests separately with a new API Key\n",
    "not_trending_us_df = process_youtube_requests(US_trending_videoIds)\n",
    "not_trending_ca_df = process_youtube_requests(CA_trending_videoIds)\n",
    "not_trending_de_df = process_youtube_requests(DE_trending_videoIds)\n",
    "not_trending_fr_df = process_youtube_requests(FR_trending_videoIds)\n",
    "not_trending_gb_df = process_youtube_requests(GB_trending_videoIds)\n",
    "not_trending_in_df = process_youtube_requests(IN_trending_videoIds)\n",
    "not_trending_jp_df = process_youtube_requests(JP_trending_videoIds)\n",
    "not_trending_kr_df = process_youtube_requests(KR_trending_videoIds)\n",
    "not_trending_mx_df = process_youtube_requests(MX_trending_videoIds)\n",
    "not_trending_ru_df = process_youtube_requests(RU_trending_videoIds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## written by Aravind Patnam\n",
    "\n",
    "## puts all nontrending and all dfs that we have together for one big df and multiple smaller ones\n",
    "list_of_all_nontrending_dfs = [not_trending_us_df, not_trending_ca_df, not_trending_de_df, \n",
    "                              not_trending_fr_df, not_trending_gb_df, not_trending_in_df, not_trending_jp_df,\n",
    "                              not_trending_kr_df, not_trending_mx_df, not_trending_ru_df]\n",
    "\n",
    "\n",
    "full_nontrending_df = pd.concat(list_of_all_nontrending_dfs)\n",
    "allDfsList = list_of_all_trending_dfs + list_of_all_nontrending_dfs + [full_trending_df] + [full_nontrending_df]\n",
    "allDfsDf = pd.concat(allDfsList)\n",
    "allDfsList.append(allDfsDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## written by Jeremy Tan\n",
    "\n",
    "## inserts category field into each dataframe from given json file\n",
    "def insert_category_field(df):\n",
    "    # add a category column using the category id from the json file\n",
    "    id_to_category = {}\n",
    "    with open(\"data/US_category_id.json\", 'r') as f: # the other category json files have missing category ids. We decided to just use the US file since it contained all of them and since they are standard internationally. \n",
    "        data = json.load(f)\n",
    "        for category in data['items']:\n",
    "            id_to_category[category['id']] = category['snippet']['title']\n",
    "        categories = []\n",
    "        for id in list(df['category_id']):\n",
    "            categories.append(id_to_category[str(id)])\n",
    "        df.insert(4, 'category', categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## written by Jeremy Tan\n",
    "## insert new category field into dataframes\n",
    "for df in allDfsList:\n",
    "    insert_category_field(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## written by Jeremy Tan\n",
    "\n",
    "## converts all columns in dataframes to type for analysis\n",
    "for df in list_of_all_nontrending_dfs:\n",
    "    df['video_id'] = df['video_id'].astype(str) \n",
    "    df['title'] = df['title'].astype(str)\n",
    "    df['channel_title'] = df['channel_title'].astype(str)\n",
    "    df['category_id'] = df['category_id'].astype(int)\n",
    "    df['category'] = df['category'].astype(str)\n",
    "    df['tags'] = df['tags'].astype(str)\n",
    "    df['views'] = df['views'].astype(int)\n",
    "    df['likes'] = df['likes'].astype(int)\n",
    "    df['dislikes'] = df['dislikes'].astype(int)\n",
    "    df['comment_count'] = df['comment_count'].astype(int)\n",
    "    df['description'] = df['description'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## written by Aravind Patnam\n",
    "\n",
    "## method to find stats about a given country df\n",
    "def find_stats(df):\n",
    "    views = df['views']\n",
    "    likes = df['likes']\n",
    "    dislikes = df['dislikes']\n",
    "    comment_count = df['comment_count']\n",
    "    tags = list(df['tags'])\n",
    "    tagsList = []\n",
    "    for t in tags: \n",
    "        tagsList.append(len(t.split('|')))\n",
    "    viewsStats = [views.sum(), views.sum() / len(views), views.min(), views.max()]\n",
    "    likesStats = [likes.sum(), likes.sum() / len(likes), likes.min(), likes.max()]\n",
    "    dislikesStats = [dislikes.sum(), dislikes.sum() / len(dislikes), dislikes.min(), dislikes.max()]\n",
    "    commentsStats = [comment_count.sum(), comment_count.sum() / len(comment_count), comment_count.min(), comment_count.max()]\n",
    "    tagsStats = [sum(tagsList), sum(tagsList) / len(tagsList), min(tagsList), max(tagsList)]\n",
    "    statsDf = pd.DataFrame({'views': viewsStats, 'likes': likesStats, 'dislikes': dislikesStats, 'comment_count': commentsStats, 'tags': tagsStats}, index = ['count', 'mean', 'min', 'max']) \n",
    "    return statsDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## written by Aravind Patnam\n",
    "\n",
    "## output is a map containing all the numeric data for each df\n",
    "describes = []\n",
    "for df in allDfsList:\n",
    "    describes.append(find_stats(df))\n",
    "describesKeys = ['US_trending_df', 'CA_trending_df', 'DE_trending_df', 'FR_trending_df', 'GB_trending_df',\n",
    "                'IN_trending_df', 'JP_trending_df', 'KR_trending_df', 'MX_trending_df', 'RU_trending_df',\n",
    "                 'not_trending_us_df', 'not_trending_ca_df', 'not_trending_de_df', 'not_trending_fr_df',\n",
    "                'not_trending_gb_df', 'not_trending_in_df', 'not_trending_jp_df', 'not_trending_kr_df',\n",
    "                 'not_trending_mx_df', 'not_trending_ru_df',\n",
    "                'full_trending_df', 'full_nontrending_df', 'allDfsDf']\n",
    "describeMap = dict(zip(describesKeys, describes))\n",
    "describeMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## written by Aravind Patnam\n",
    "\n",
    "## make sense of each data point we have from the describeMap and clean it for visualization\n",
    "countries = list(describeMap.keys())\n",
    "views_count = []\n",
    "views_mean = []\n",
    "views_max = []\n",
    "views_min = []\n",
    "likes_count = []\n",
    "likes_mean = []\n",
    "likes_max = []\n",
    "likes_min = []\n",
    "dislikes_count = []\n",
    "dislikes_mean = []\n",
    "dislikes_max = []\n",
    "dislikes_min = []\n",
    "\n",
    "comments_mean = []\n",
    "comments_max = []\n",
    "comments_min = []\n",
    "\n",
    "tags_mean = []\n",
    "tags_max = []\n",
    "tags_min = []\n",
    "\n",
    "for a in countries:\n",
    "    views_count.append(describeMap[a].loc['count', 'views'])\n",
    "    views_mean.append(describeMap[a].loc['mean', 'views'])\n",
    "    views_max.append(describeMap[a].loc['max', 'views'])\n",
    "    views_min.append(describeMap[a].loc['min', 'views'])\n",
    "    \n",
    "    likes_count.append(describeMap[a].loc['count', 'likes'])\n",
    "    likes_mean.append(describeMap[a].loc['mean', 'likes'])\n",
    "    likes_max.append(describeMap[a].loc['max', 'likes'])\n",
    "    likes_min.append(describeMap[a].loc['min', 'likes'])\n",
    "    \n",
    "    dislikes_count.append(describeMap[a].loc['count', 'dislikes'])\n",
    "    dislikes_mean.append(describeMap[a].loc['mean', 'dislikes'])\n",
    "    dislikes_max.append(describeMap[a].loc['max', 'dislikes'])\n",
    "    dislikes_min.append(describeMap[a].loc['min', 'dislikes'])\n",
    "    \n",
    "    comments_mean.append(describeMap[a].loc['mean', 'comment_count'])\n",
    "    comments_max.append(describeMap[a].loc['max', 'comment_count'])\n",
    "    comments_min.append(describeMap[a].loc['min', 'comment_count'])\n",
    "    \n",
    "    tags_mean.append(describeMap[a].loc['mean', 'tags'])\n",
    "    tags_max.append(describeMap[a].loc['max', 'tags'])\n",
    "    tags_min.append(describeMap[a].loc['min', 'tags'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hover over the bar graphs to play with them and see what each one represents!! To show the scale of these numbers better, the graph only goes till 10000. A numeric analysis amongst likes, views, dislikes, comments, and tags is presented. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## written by Aravind Patnam\n",
    "\n",
    "# visualize above numeric data\n",
    "bokeh.io.reset_output()\n",
    "bokeh.io.output_notebook()\n",
    "\n",
    "\n",
    "\n",
    "countries = describesKeys\n",
    "categories = ['views_mean', 'views_max', 'views_min']\n",
    "categoriesLikes = ['likes_mean', 'likes_max', 'likes_min']\n",
    "categoriesDislikes = ['dislikes_mean', 'dislikes_max', 'dislikes_min']\n",
    "categoriesComments = ['comments_mean', 'comments_max', 'comments_min']\n",
    "categoriesTags = ['tags_mean', 'tags_max', 'tags_min']\n",
    "\n",
    "data = {'countries' : countries,\n",
    "        'views_mean': views_mean,\n",
    "        'views_max':views_max, \n",
    "        'views_min':views_min }\n",
    "dataLikes = {'countries' : countries, 'likes_mean' : likes_mean, 'likes_max': likes_max, 'likes_min' : likes_min}\n",
    "\n",
    "dataDislikes = {'countries' : countries, 'dislikes_mean' : dislikes_mean, 'dislikes_max' : dislikes_max, 'dislikes_min': dislikes_min}\n",
    "\n",
    "dataComments = {'countries' : countries, 'comments_mean' : comments_mean, 'comments_max' : comments_max, 'comments_min': comments_min}\n",
    "\n",
    "dataTags = {'countries' : countries, 'tags_mean' : tags_mean, 'tags_max': tags_max, 'tags_min' : tags_min}\n",
    "\n",
    "# this creates [ (\"Apples\", \"2015\"), (\"Apples\", \"2016\"), (\"Apples\", \"2017\"), (\"Pears\", \"2015), ... ]\n",
    "x = [ (c, category) for c in countries for category in categories ]\n",
    "# print (len(x))\n",
    "yLikes = [(c, l) for c in countries for l in categoriesLikes]\n",
    "yDislikes = [(c, h) for c in countries for h in categoriesDislikes]\n",
    "\n",
    "yComments = [(c, comms) for c in countries for comms in categoriesComments]\n",
    "\n",
    "yTags = [(c, ts) for c in countries for ts in categoriesTags]\n",
    "\n",
    "counts = sum(zip(data['views_mean'], data['views_max'], data['views_min']), ()) # like an hstack\n",
    "countsLikes = sum(zip(dataLikes['likes_mean'], dataLikes['likes_max'], dataLikes['likes_min']), ()) # like an hstack\n",
    "countsDislikes = sum(zip(dataDislikes['dislikes_mean'], dataDislikes['dislikes_max'], dataDislikes['dislikes_min']), ())\n",
    "countsComments = sum(zip(dataComments['comments_mean'], dataComments['comments_max'], dataComments['comments_min']), ())\n",
    "\n",
    "countsTags = sum(zip(dataTags['tags_mean'], dataTags['tags_max'], dataTags['tags_min']), ())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "source = ColumnDataSource(data=dict(x=x, counts=counts))\n",
    "sourceLikes = ColumnDataSource(data=dict(x=yLikes, counts=countsLikes))\n",
    "sourceDislikes = ColumnDataSource(data=dict(x=yDislikes, counts=countsDislikes))\n",
    "\n",
    "sourceComments = ColumnDataSource(data=dict(x=yComments, counts=countsComments))\n",
    "\n",
    "sourceTags = ColumnDataSource(data=dict(x=yTags, counts=countsTags))\n",
    "\n",
    "\n",
    "hover = HoverTool()\n",
    "\n",
    "hover.tooltips = [\n",
    "    (\"(x,y)\", \"($x, $y)\"),\n",
    "    (\"Country\", \"@x\"),\n",
    "    (\"Stat\" , \"@counts\")\n",
    "]\n",
    "\n",
    "p = figure(x_range=FactorRange(*x), plot_height=250, plot_width=3000, title=\"Views Counts Per Country Trending/NonTrending\",\n",
    "           toolbar_location=None, tools = [BoxSelectTool(), LassoSelectTool(), ResetTool(), hover, TapTool(), PolySelectTool()])\n",
    "\n",
    "p.vbar(x='x', top='counts', width=0.9, source=source, line_color=\"white\",\n",
    "       fill_color=factor_cmap('x', palette=Spectral6, factors=categories, start=1, end=2))\n",
    "\n",
    "\n",
    "p1 = figure(x_range=FactorRange(*yLikes), plot_height=250, plot_width=3000, title=\"Likes Counts Per Country Trending/NonTrending\",\n",
    "           toolbar_location=None, tools = [BoxSelectTool(), LassoSelectTool(), ResetTool(), hover, TapTool(), PolySelectTool()])\n",
    "\n",
    "p1.vbar(x='x', top='counts', width=0.9, source=sourceLikes, line_color=\"white\",\n",
    "       fill_color=factor_cmap('x', palette=Spectral6, factors=categoriesLikes, start=1, end=2))\n",
    "\n",
    "\n",
    "p2 = figure(x_range=FactorRange(*yDislikes), plot_height=250, plot_width=3000, title=\"Dislikes Counts Per Country Trending/NonTrending\",\n",
    "           toolbar_location=None, tools = [BoxSelectTool(), LassoSelectTool(), ResetTool(), hover, TapTool(), PolySelectTool()])\n",
    "\n",
    "p2.vbar(x='x', top='counts', width=0.9, source=sourceDislikes, line_color=\"white\",\n",
    "       fill_color=factor_cmap('x', palette=Spectral6, factors=categoriesDislikes, start=1, end=2))\n",
    "\n",
    "\n",
    "\n",
    "p3 = figure(x_range=FactorRange(*yComments), plot_height=250, plot_width=3000, title=\"Comment Counts Per Country Trending/NonTrending\",\n",
    "           toolbar_location=None, tools = [BoxSelectTool(), LassoSelectTool(), ResetTool(), hover, TapTool(), PolySelectTool()])\n",
    "\n",
    "p3.vbar(x='x', top='counts', width=0.9, source=sourceComments, line_color=\"white\",\n",
    "       fill_color=factor_cmap('x', palette=Spectral6, factors=categoriesComments, start=1, end=2))\n",
    "\n",
    "\n",
    "p4 = figure(x_range=FactorRange(*yTags), plot_height=250, plot_width=3000, title=\"Tags Counts Per Country Trending/NonTrending\",\n",
    "           toolbar_location=None, tools = [BoxSelectTool(), LassoSelectTool(), ResetTool(), hover, TapTool(), PolySelectTool()])\n",
    "\n",
    "p4.vbar(x='x', top='counts', width=0.9, source=sourceTags, line_color=\"white\",\n",
    "       fill_color=factor_cmap('x', palette=Spectral6, factors=categoriesTags, start=1, end=2))\n",
    "\n",
    "p.y_range.start = 0\n",
    "p.y_range.end = 10000\n",
    "p.x_range.range_padding = 0.1\n",
    "p.xaxis.major_label_orientation = 1\n",
    "p.xgrid.grid_line_color = None\n",
    "\n",
    "p1.y_range.start = 0\n",
    "p1.y_range.end = 10000\n",
    "p1.x_range.range_padding = 0.1\n",
    "p1.xaxis.major_label_orientation = 1\n",
    "p1.xgrid.grid_line_color = None\n",
    "\n",
    "p2.y_range.start = 0\n",
    "p2.y_range.end = 10000\n",
    "p2.x_range.range_padding = 0.1\n",
    "p2.xaxis.major_label_orientation = 1\n",
    "p2.xgrid.grid_line_color = None\n",
    "\n",
    "\n",
    "p3.y_range.start = 0\n",
    "p3.y_range.end = 10000\n",
    "p3.x_range.range_padding = 0.1\n",
    "p3.xaxis.major_label_orientation = 1\n",
    "p3.xgrid.grid_line_color = None\n",
    "\n",
    "p4.y_range.start = 0\n",
    "p4.y_range.end = 100\n",
    "p4.x_range.range_padding = 0.1\n",
    "p4.xaxis.major_label_orientation = 1\n",
    "p4.xgrid.grid_line_color = None\n",
    "\n",
    "\n",
    "# Bokeh Library\n",
    "from bokeh.io import output_file\n",
    "from bokeh.models.widgets import Tabs, Panel\n",
    "\n",
    "\n",
    "\n",
    "# Create two panels, one for each conference\n",
    "pPanel = Panel(child=p, title='Views Analysis')\n",
    "p1Panel = Panel(child=p1, title='Likes Analysis')\n",
    "p2Panel = Panel(child=p2, title='Dislikes Analysis')\n",
    "p3Panel = Panel(child=p3, title='Comments Analysis')\n",
    "p4Panel = Panel(child=p4, title='Tags Analysis')\n",
    "\n",
    "# Assign the panels to Tabs\n",
    "tabs = Tabs(tabs=[pPanel, p1Panel, p2Panel, p3Panel, p4Panel])\n",
    "\n",
    "# Show the tabbed layout\n",
    "show(tabs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hover over the bar graphs to play with them and see what each one represents!! Each one shows the likes-dislikes ratio amongst trending and non-trending videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## written by Aravind Patnam\n",
    "\n",
    "## visualizes likes to dislikes ratio from above numeric data\n",
    "\n",
    "likesToDislikesRatio = [i / j for i, j in zip(likes_count, dislikes_count)] \n",
    "# Bokeh libraries\n",
    "\n",
    "\n",
    "bokeh.io.reset_output()\n",
    "bokeh.io.output_notebook()\n",
    "colorlist = Category20c[13] + Plasma[10]\n",
    "\n",
    "hover = HoverTool()\n",
    "\n",
    "hover.tooltips = [\n",
    "    (\"Country\", \"@countries\"),\n",
    "    (\"Stat\" , \"@likesToDislikesRatio\")\n",
    "]\n",
    "\n",
    "source = ColumnDataSource(data=dict(countries=countries, likesToDislikesRatio=likesToDislikesRatio))\n",
    "pRatio = figure(x_range=countries, plot_height=250, plot_width=2500, toolbar_location=None, title=\"Likes To Dislikes\", tools = [hover])\n",
    "pRatio.vbar(x='countries', top='likesToDislikesRatio', width=0.9, source=source, \n",
    "           line_color='white', fill_color=factor_cmap('countries', palette=colorlist, factors=countries))\n",
    "\n",
    "pRatio.xgrid.grid_line_color = None\n",
    "pRatio.y_range.start = 0\n",
    "pRatio.y_range.end = 50\n",
    "pRatio.legend.orientation = \"horizontal\"\n",
    "pRatio.legend.location = \"top_center\"\n",
    "\n",
    "show(pRatio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## written by Aravind Patnam\n",
    "\n",
    "###get most common tags from a country df\n",
    "def get_most_common_tags(country_df):\n",
    "    tags = country_df['tags'].to_string(index=False, header=False)\n",
    "    split_tags = [i.replace('\"', '') for i in tags.split(\"|\")]\n",
    "    stop_words = stopwords.words('english')\n",
    "    filtered_tags = [word for word in split_tags if word not in stop_words]\n",
    "    fdist = FreqDist(split_tags)\n",
    "    most_popular_tags = fdist.most_common(1000)\n",
    "    return dict(most_popular_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## written by Aravind Patnam\n",
    "\n",
    "## a bunch of maps containing the most popular tags for each country that are both trending and not trending and their frequency\n",
    "us_trending_most_common_tags = get_most_common_tags(US_trending_df)\n",
    "ca_trending_most_common_tags = get_most_common_tags(CA_trending_df)\n",
    "de_trending_most_common_tags = get_most_common_tags(DE_trending_df)\n",
    "fr_trending_most_common_tags = get_most_common_tags(FR_trending_df)\n",
    "gb_trending_most_common_tags = get_most_common_tags(GB_trending_df)\n",
    "in_trending_most_common_tags = get_most_common_tags(IN_trending_df)\n",
    "jp_trending_most_common_tags = get_most_common_tags(JP_trending_df)\n",
    "kr_trending_most_common_tags = get_most_common_tags(KR_trending_df)\n",
    "mx_trending_most_common_tags = get_most_common_tags(MX_trending_df)\n",
    "ru_trending_most_common_tags = get_most_common_tags(RU_trending_df)\n",
    "\n",
    "us_nontrending_most_common_tags = get_most_common_tags(not_trending_us_df)\n",
    "ca_nontrending_most_common_tags = get_most_common_tags(not_trending_ca_df)\n",
    "de_nontrending_most_common_tags = get_most_common_tags(not_trending_de_df)\n",
    "fr_nontrending_most_common_tags = get_most_common_tags(not_trending_fr_df)\n",
    "gb_nontrending_most_common_tags = get_most_common_tags(not_trending_gb_df)\n",
    "in_nontrending_most_common_tags = get_most_common_tags(not_trending_in_df)\n",
    "jp_nontrending_most_common_tags = get_most_common_tags(not_trending_jp_df)\n",
    "kr_nontrending_most_common_tags = get_most_common_tags(not_trending_kr_df)\n",
    "mx_nontrending_most_common_tags = get_most_common_tags(not_trending_mx_df)\n",
    "ru_nontrending_most_common_tags = get_most_common_tags(not_trending_ru_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a wordcloud visualization of some of the countries and a comparison of them between trending and non-trending videos. Since there is not much space to do all 20 different combinations, only the 3 with the most popular tags were displayed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## written by Aravind Patnam\n",
    "\n",
    "\n",
    "## wordcloud visualization on countries with most top tags\n",
    "\n",
    "links = ['https://cdn.pixabay.com/photo/2017/03/14/21/00/american-flag-2144392_960_720.png', \n",
    "        'https://i.pinimg.com/originals/93/85/dd/9385dde0f8cf96e0c60e5e659036b303.jpg',\n",
    "        'https://cdn.britannica.com/97/1597-004-7C2918C6/Flag-India.jpg']\n",
    "\n",
    "fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(20, 20))\n",
    "mask_USA = np.array(Image.open(requests.get(links[0], stream=True).raw))\n",
    "mask_GB = np.array(Image.open(requests.get(links[1], stream=True).raw))\n",
    "mask_IN = np.array(Image.open(requests.get(links[2], stream=True).raw))\n",
    "image_colors_usa = ImageColorGenerator(mask_USA)\n",
    "image_colors_gb = ImageColorGenerator(mask_GB)\n",
    "image_colors_in = ImageColorGenerator(mask_IN)\n",
    "wordcloud_usa_trending = WordCloud(width=1000, height=1000, mask=mask_USA, background_color=\"white\", max_words=1000, max_font_size=1000).generate(str(list(us_trending_most_common_tags.keys())))\n",
    "wordcloud_gb_trending = WordCloud(width=1000, height=1000, mask=mask_GB, background_color=\"white\", max_words=1000, max_font_size=1000).generate(str(list(gb_trending_most_common_tags.keys())))\n",
    "wordcloud_in_trending = WordCloud(width=1000, height=1000, mask=mask_IN, background_color=\"white\", max_words=1000, max_font_size=1000).generate(str(list(in_trending_most_common_tags.keys())))\n",
    "\n",
    "\n",
    "wordcloud_usa_nontrending = WordCloud(width=1000, height=1000, mask=mask_USA, background_color=\"white\", max_words=1000, max_font_size=1000).generate(str(list(us_nontrending_most_common_tags.keys())))\n",
    "wordcloud_gb_nontrending = WordCloud(width=1000, height=1000, mask=mask_GB, background_color=\"white\", max_words=1000, max_font_size=1000).generate(str(list(gb_nontrending_most_common_tags.keys())))\n",
    "wordcloud_in_nontrending = WordCloud(width=1000, height=1000, mask=mask_IN, background_color=\"white\", max_words=1000, max_font_size=1000).generate(str(list(in_nontrending_most_common_tags.keys())))\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20,20), facecolor='k')\n",
    "\n",
    "axes[0, 0].imshow(wordcloud_usa_trending.recolor(color_func=image_colors_usa), interpolation=\"bilinear\")\n",
    "axes[0, 0].set_title(\"USA Trending Tags\")\n",
    "axes[0, 0].axis(\"off\")\n",
    "\n",
    "axes[1, 0].imshow(wordcloud_gb_trending.recolor(color_func=image_colors_gb), interpolation=\"bilinear\")\n",
    "axes[1, 0].set_title(\"Great Britain Trending Tags\")\n",
    "axes[1, 0].axis(\"off\")\n",
    "\n",
    "\n",
    "axes[2, 0].imshow(wordcloud_in_trending.recolor(color_func=image_colors_in), interpolation=\"bilinear\")\n",
    "axes[2, 0].set_title(\"India Trending Tags\")\n",
    "axes[2, 0].axis(\"off\")\n",
    "\n",
    "axes[0, 1].imshow(wordcloud_usa_nontrending.recolor(color_func=image_colors_usa), interpolation=\"bilinear\")\n",
    "axes[0, 1].set_title(\"USA Non-Trending Tags\")\n",
    "axes[0, 1].axis(\"off\")\n",
    "\n",
    "\n",
    "axes[1, 1].imshow(wordcloud_gb_nontrending.recolor(color_func=image_colors_gb), interpolation=\"bilinear\")\n",
    "axes[1, 1].set_title(\"Great Britain Non-Trending Tags\")\n",
    "axes[1, 1].axis(\"off\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "axes[2, 1].imshow(wordcloud_in_nontrending.recolor(color_func=image_colors_in), interpolation=\"bilinear\")\n",
    "axes[2, 1].set_title(\"India Non-Trending Tags\")\n",
    "axes[2, 1].axis(\"off\")\n",
    "\n",
    "\n",
    "\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below represents a sentiment analysis conducted on the youtube tags. A datset of positive words and negative words is provided for the model to work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## written by Aravind Patnam\n",
    "\n",
    "## do sentiment analysis on each of the tags\n",
    "## return classifications on each of the tags in both trending and non-trending per country\n",
    "positive_words_df = pd.read_fwf('data/positivewords.txt')\n",
    "negative_words_df = pd.read_fwf('data/negativewords.txt')\n",
    "\n",
    "def extract_features(words):\n",
    "    return dict([(word, True) for word in words.split()])\n",
    "\n",
    "def build_sentiment_analysis_model():\n",
    "    positive_words = positive_words_df['positivewords'].values.tolist()\n",
    "    negative_words = negative_words_df['negativewords'].values.tolist()\n",
    "    pos_feats = [(extract_features(f), 'positive') for f in positive_words ]\n",
    "    neg_feats = [(extract_features(f), 'negative') for f in negative_words ]\n",
    "    dataset = pos_feats + neg_feats\n",
    "    random.shuffle(dataset)\n",
    "    cutoff = int(0.80 * len(dataset))\n",
    "    train_data = dataset[:cutoff]\n",
    "    test_data = dataset[cutoff:]\n",
    "\n",
    "    classifier = NaiveBayesClassifier.train(train_data)\n",
    "    print(\"Accuracy is:\", classify.accuracy(classifier, test_data))\n",
    "    # print(classifier.show_most_informative_features(10))\n",
    "    return classifier\n",
    "\n",
    "def execute_model(tags):\n",
    "    classifications = {}\n",
    "    classifier = build_sentiment_analysis_model()\n",
    "    # classifier.show_most_informative_features(5)\n",
    "    for tag in tags:\n",
    "        classified = classifier.classify(extract_features(tag))\n",
    "        classifications[tag] = classified\n",
    "    return classifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the accuracy with which the model runs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## written by Aravind Patnam\n",
    "\n",
    "## gets the classifications from the sentiment analysis and prints out accuracies of the model\n",
    "classifications_us_trending = execute_model(list(us_trending_most_common_tags.keys()))\n",
    "classifications_ca_trending = execute_model(list(ca_trending_most_common_tags.keys()))\n",
    "classifications_de_trending = execute_model(list(de_trending_most_common_tags.keys()))\n",
    "classifications_fr_trending = execute_model(list(fr_trending_most_common_tags.keys()))\n",
    "classifications_gb_trending = execute_model(list(gb_trending_most_common_tags.keys()))\n",
    "classifications_in_trending = execute_model(list(in_trending_most_common_tags.keys()))\n",
    "classifications_jp_trending = execute_model(list(jp_trending_most_common_tags.keys()))\n",
    "classifications_kr_trending = execute_model(list(kr_trending_most_common_tags.keys()))\n",
    "classifications_mx_trending = execute_model(list(mx_trending_most_common_tags.keys()))\n",
    "classifications_ru_trending = execute_model(list(ru_trending_most_common_tags.keys()))\n",
    "\n",
    "classifications_us_nontrending = execute_model(list(us_nontrending_most_common_tags.keys()))\n",
    "classifications_ca_nontrending = execute_model(list(ca_nontrending_most_common_tags.keys()))\n",
    "classifications_de_nontrending = execute_model(list(de_nontrending_most_common_tags.keys()))\n",
    "classifications_fr_nontrending = execute_model(list(fr_nontrending_most_common_tags.keys()))\n",
    "classifications_gb_nontrending = execute_model(list(gb_nontrending_most_common_tags.keys()))\n",
    "classifications_in_nontrending = execute_model(list(in_nontrending_most_common_tags.keys()))\n",
    "classifications_jp_nontrending = execute_model(list(jp_nontrending_most_common_tags.keys()))\n",
    "classifications_kr_nontrending = execute_model(list(kr_nontrending_most_common_tags.keys()))\n",
    "classifications_mx_nontrending = execute_model(list(mx_nontrending_most_common_tags.keys()))\n",
    "classifications_ru_nontrending = execute_model(list(ru_nontrending_most_common_tags.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## written by Aravind Patnam\n",
    "\n",
    "## pulls actual statistics from classifications for visualization\n",
    "def get_sentiment_stats(classification, country):\n",
    "    sentiments = list(classification.values())\n",
    "    sentiments_df = pd.DataFrame(sentiments, columns=['Sentiment'])\n",
    "    negatives = len(sentiments_df[sentiments_df['Sentiment'] =='negative'])\n",
    "    positives = len(sentiments_df[sentiments_df['Sentiment'] =='positive'])\n",
    "    total_len = len(sentiments_df)\n",
    "    percentage_of_negative = negatives / total_len * 100\n",
    "    percentage_of_positive = positives / total_len * 100\n",
    "    ratio = positives/negatives\n",
    "    # ratioStr = \"{} positive/negative ratio: {} -----> {}% positives of total , {}% negatives of total\".format(country, ratio, percentage_of_positive, percentage_of_negative)\n",
    "    # print (ratioStr)\n",
    "    return country, positives, negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## written by Aravind Patnam\n",
    "\n",
    "## calls above method for stats for each country for visualization\n",
    "\n",
    "country_us_trending, us_trending_pos, us_trending_neg = get_sentiment_stats(classifications_us_trending, \"USA_Trending\")\n",
    "country_ca_trending, ca_trending_pos, ca_trending_neg = get_sentiment_stats(classifications_ca_trending, \"Canada_Trending\")\n",
    "country_de_trending, de_trending_pos, de_trending_neg = get_sentiment_stats(classifications_de_trending, \"Denmark_Trending\")\n",
    "country_fr_trending, fr_trending_pos, fr_trending_neg = get_sentiment_stats(classifications_fr_trending, \"France_Trending\")\n",
    "country_gb_trending, gb_trending_pos, gb_trending_neg = get_sentiment_stats(classifications_gb_trending, \"GreatBritain_Trending\")\n",
    "country_in_trending, in_trending_pos, in_trending_neg = get_sentiment_stats(classifications_in_trending, \"India_Trending\")\n",
    "country_jp_trending, jp_trending_pos, jp_trending_neg = get_sentiment_stats(classifications_jp_trending, \"Japan_Trending\")\n",
    "country_kr_trending, kr_trending_pos, kr_trending_neg = get_sentiment_stats(classifications_kr_trending, \"SouthKorea_Trending\")\n",
    "country_mx_trending, mx_trending_pos, mx_trending_neg = get_sentiment_stats(classifications_mx_trending, \"Mexico_Trending\")\n",
    "country_ru_trending, ru_trending_pos, ru_trending_neg = get_sentiment_stats(classifications_ru_trending, \"Russia_Trending\")\n",
    "\n",
    "\n",
    "country_us_nontrending, us_nontrending_pos, us_nontrending_neg = get_sentiment_stats(classifications_us_nontrending, \"USA_NonTrending\")\n",
    "country_ca_nontrending, ca_nontrending_pos, ca_nontrending_neg = get_sentiment_stats(classifications_ca_nontrending, \"Canada_NonTrending\")\n",
    "country_de_nontrending, de_nontrending_pos, de_nontrending_neg = get_sentiment_stats(classifications_de_nontrending, \"Denmark_NonTrending\")\n",
    "country_fr_nontrending, fr_nontrending_pos, fr_nontrending_neg = get_sentiment_stats(classifications_fr_nontrending, \"France_NonTrending\")\n",
    "country_gb_nontrending, gb_nontrending_pos, gb_nontrending_neg = get_sentiment_stats(classifications_gb_nontrending, \"GreatBritain_NonTrending\")\n",
    "country_in_nontrending, in_nontrending_pos, in_nontrending_neg = get_sentiment_stats(classifications_in_nontrending, \"India_NonTrending\")\n",
    "country_jp_nontrending, jp_nontrending_pos, jp_nontrending_neg = get_sentiment_stats(classifications_jp_nontrending, \"Japan_NonTrending\")\n",
    "country_kr_nontrending, kr_nontrending_pos, kr_nontrending_neg = get_sentiment_stats(classifications_kr_nontrending, \"SouthKorea_NonTrending\")\n",
    "country_mx_nontrending, mx_nontrending_pos, mx_nontrending_neg = get_sentiment_stats(classifications_mx_nontrending, \"Mexico_NonTrending\")\n",
    "country_ru_nontrending, ru_nontrending_pos, ru_nontrending_neg = get_sentiment_stats(classifications_ru_nontrending, \"Russia_NonTrending\")\n",
    "\n",
    "countries = [country_us_trending, country_ca_trending, country_de_trending, country_fr_trending, country_gb_trending,\n",
    "            country_in_trending, country_jp_trending, country_kr_trending, country_mx_trending, country_ru_trending,\n",
    "            country_us_nontrending, country_ca_nontrending, country_de_nontrending, country_fr_nontrending, \n",
    "            country_gb_nontrending, country_in_nontrending, country_jp_nontrending, country_kr_nontrending, \n",
    "            country_mx_nontrending, country_ru_nontrending]\n",
    "positivePercenteages = [us_trending_pos, ca_trending_pos, de_trending_pos, fr_trending_pos, gb_trending_pos, in_trending_pos, jp_trending_pos,\n",
    "            kr_trending_pos, mx_trending_pos, ru_trending_pos, us_nontrending_pos, ca_nontrending_pos, de_nontrending_pos, fr_nontrending_pos,\n",
    "            gb_nontrending_pos, in_nontrending_pos, jp_nontrending_pos, kr_nontrending_pos, mx_nontrending_pos, ru_nontrending_pos]\n",
    "negativePercentages = [us_trending_neg, ca_trending_neg, de_trending_neg, fr_trending_neg, gb_trending_neg, in_trending_neg, jp_trending_neg, \n",
    "                      kr_trending_neg, mx_trending_neg, ru_trending_neg, us_nontrending_neg, ca_nontrending_neg, de_nontrending_neg, \n",
    "                      fr_nontrending_neg, gb_nontrending_neg, in_nontrending_neg, jp_nontrending_neg, kr_nontrending_neg, mx_nontrending_neg,\n",
    "                      ru_nontrending_neg]\n",
    "posToNegRatios = [i / j for i, j in zip(positivePercenteages, negativePercentages)] \n",
    "data_dict = {\"Country\" : countries, \"Positives\": positivePercenteages, \"Negatives\": negativePercentages, \"PositiveNegativeRatio\": posToNegRatios}\n",
    "data = pd.DataFrame(data_dict, columns = ['Country', 'Positives' , 'Negatives', 'PositiveNegativeRatio'])\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hover over the plots and switch table to play with them and see what each one represents!! Use the toolbar to crop out some plots for better analysis. The first plot visualizes the ratios between the positive and negative tags while the second one shows the actual values that were classified as positive and negative by the sentiment analysis classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## written by Aravind Patnam\n",
    "\n",
    "## visualizes sentiment analysis ratios\n",
    "\n",
    "## import pandas as pd\n",
    "\n",
    "\n",
    "bokeh.io.reset_output()\n",
    "bokeh.io.output_notebook()\n",
    "mySource = bp.ColumnDataSource(data)\n",
    "#Use the field name of the column source\n",
    "mapper = linear_cmap(field_name='Positives', palette=\"Spectral6\" ,low=max(list(data['Positives']) + list(data['Negatives'])) ,high=min(list(data['Positives']) + list(data['Negatives'])))\n",
    "\n",
    "hover = HoverTool()\n",
    "hover.tooltips = [\n",
    "    (\"Country\", \"@Country\"),\n",
    "    (\"Ratio\" , \"@PositiveNegativeRatio\"),\n",
    "    (\"Positives\", \"@Positives\"),\n",
    "    (\"Negatives\" , \"@Negatives\")\n",
    "]\n",
    "myPlot = bp.figure(\n",
    "               title = 'Positives vs. Negatives',\n",
    "               plot_height=500,\n",
    "               plot_width=500, \n",
    "    tools = [BoxSelectTool(), LassoSelectTool(), ResetTool(), hover, TapTool(), PolySelectTool()], \n",
    "    background_fill_color = \"black\",\n",
    "    x_axis_label = 'Positives', y_axis_label = 'Negatives',\n",
    "x_range=(0, 1000), y_range=(0, 1000))\n",
    "myPlot.title.text_font_size = '20pt'\n",
    "myPlot.xaxis.axis_label_text_font_size = \"20pt\"\n",
    "myPlot.yaxis.axis_label_text_font_size = \"20pt\"\n",
    "myPlot.xaxis.ticker = SingleIntervalTicker(interval=50)\n",
    "myPlot.yaxis.ticker = SingleIntervalTicker(interval=50)\n",
    "myPlot.circle(\"Positives\",\n",
    "          \"Negatives\",\n",
    "              line_color=mapper,color=mapper, fill_alpha=1, \n",
    "          source=mySource,\n",
    "          size = 12,\n",
    "             selection_color='deepskyblue',\n",
    "           nonselection_color='lightgray',\n",
    "             hover_fill_color='yellow', hover_alpha=2.0)\n",
    "color_bar = ColorBar(color_mapper=mapper['transform'], width=8,  location=(0,0))\n",
    "\n",
    "myPlot.add_layout(color_bar, 'right')\n",
    "\n",
    "\n",
    "\n",
    "bokeh.io.reset_output()\n",
    "bokeh.io.output_notebook()\n",
    "\n",
    "\n",
    "countries = list(data['Country'])\n",
    "hover = HoverTool(\n",
    "    tooltips=[\n",
    "        (\"Ratio\", \"@top\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "colors = ['red', 'red', 'red', 'red',\n",
    "          'red', 'red', 'red', 'red',\n",
    "          'red', 'red', 'green', 'green', 'green', 'green',\n",
    "          'green', 'green', 'green', 'green',\n",
    "          'green', 'green']\n",
    "\n",
    "\n",
    "p = figure(x_range=countries, plot_height=500, title=\"Sentiment Analysis Ratios\", width = 500,  background_fill_color = \"black\")\n",
    "legends = ['Trending', 'Not Trending']\n",
    "v = p.vbar(x=countries, top=list(data['PositiveNegativeRatio']), width=0.5, color=colors)\n",
    "v1 = p.vbar(x=countries, top=list(data['PositiveNegativeRatio']), width=0.5, color='green')\n",
    "p.add_layout(Legend(items=[\n",
    "        (\"Trending\"   , [v]),\n",
    "        (\"NonTrending\" , [v1]),\n",
    "    ]))\n",
    "v1.visible = False\n",
    "\n",
    "p.add_tools(hover)\n",
    "p.xgrid.grid_line_color = None\n",
    "p.y_range.start = 0\n",
    "p.xaxis.major_label_orientation = -1.5\n",
    "p.xaxis.axis_label = \"Countries\"\n",
    "p.yaxis.axis_label = \"Ratio of Positive:Negative\"\n",
    "p.xgrid.grid_line_color = None\n",
    "\n",
    "p.title.text_font_size = '20pt'\n",
    "p.xaxis.axis_label_text_font_size = \"20pt\"\n",
    "p.yaxis.axis_label_text_font_size = \"20pt\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Increase the plot widths\n",
    "myPlot.plot_width = p.plot_width = 1000\n",
    "\n",
    "# Create two panels, one for each conference\n",
    "myPlotPanel = Panel(child=myPlot, title='Positives vs. Negatives')\n",
    "pPanel = Panel(child=p, title='Sentiment Analysis Ratios')\n",
    "\n",
    "# Assign the panels to Tabs\n",
    "tabs = Tabs(tabs=[pPanel, myPlotPanel])\n",
    "\n",
    "# Show the tabbed layout\n",
    "show(tabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## written by Aravind Patnam\n",
    "\n",
    "\n",
    "## lda model prep for topic model inference from youtube tags and description accross trending and nontrending videos\n",
    "\n",
    "parser = English()\n",
    "def tokenize(text):\n",
    "    lda_tokens = []\n",
    "    tokens = parser(text)\n",
    "    for token in tokens:\n",
    "        if token.orth_.isspace():\n",
    "            continue\n",
    "        elif token.like_url:\n",
    "            lda_tokens.append('URL')\n",
    "        elif token.orth_.startswith('@'):\n",
    "            lda_tokens.append('SCREEN_NAME')\n",
    "        else:\n",
    "            lda_tokens.append(token.lower_)\n",
    "    return lda_tokens\n",
    "\n",
    "def get_lemma(word):\n",
    "    lemma = wn.morphy(word)\n",
    "    if lemma is None:\n",
    "        return word\n",
    "    else:\n",
    "        return lemma\n",
    "    \n",
    "def get_lemma2(word):\n",
    "    return WordNetLemmatizer().lemmatize(word)\n",
    "\n",
    "nltk.download('stopwords')\n",
    "en_stop = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "def prepare_text_for_lda(text):\n",
    "    tokens = tokenize(str(text))\n",
    "    tokens = [token for token in tokens if len(token) > 4]\n",
    "    tokens = [token for token in tokens if token not in en_stop]\n",
    "    tokens = [get_lemma(token) for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## written by Aravind Patnam\n",
    "\n",
    "## method to execute LDA model and clean the LDA input one more time\n",
    "def do_LDA(lda_input):\n",
    "    text_data = []\n",
    "    for line in lda_input:\n",
    "        tokens = prepare_text_for_lda(line)\n",
    "        if random.random() > .99:\n",
    "            text_data.append(tokens)\n",
    "    topics, corpus, dictionary = execute_LDA(text_data)\n",
    "    return topics, corpus, dictionary\n",
    "    \n",
    "def execute_LDA(text_data):\n",
    "    dictionary = corpora.Dictionary(text_data)\n",
    "    corpus = [dictionary.doc2bow(text) for text in text_data]\n",
    "    pickle.dump(corpus, open('data/corpus.pkl', 'wb'))\n",
    "    dictionary.save('data/dictionary.gensim')\n",
    "    ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = 10, id2word=dictionary, passes=15)\n",
    "    ldamodel.save('data/model5.gensim')\n",
    "    topics = ldamodel.print_topics(num_words=10)\n",
    "    return topics, corpus, dictionary\n",
    "\n",
    "def clean_lda_input(input):\n",
    "    l = []\n",
    "    for a in input:\n",
    "        text = re.sub(r\"http\\S+\", \"\", str(a))\n",
    "        l.append(text)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## written by Aravind Patnam\n",
    "\n",
    "## visualize LDA using pyLDAvis -> this might be only visible on nbviewer depending on your notebook viewing settings\n",
    "\n",
    "def visualize_LDA(start, corpus, dictionary):\n",
    "    if (start == True):\n",
    "        lda = gensim.models.ldamodel.LdaModel.load('data/model5.gensim')\n",
    "        lda_display = pyLDAvis.gensim.prepare(lda, corpus, dictionary, sort_topics=False)\n",
    "        return lda_display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hover and play around with the principle component analysis presented below. LDA topic model was used on youtube tags and descriptions to show these findings. The relevancy metric at the time of this submission is set to around 0.30 since it showed the most favorable results that were not too specific and not too generic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## written by Aravind Patnam\n",
    "\n",
    "## this will take a long time to run!\n",
    "\n",
    "## visualize trending videos using pyLDAvis -> this might be only visible on nbviewer depending on your notebook viewing settings\n",
    "\n",
    "full_trending_lda_input = list(get_most_common_tags(full_trending_df).keys()) + list (full_trending_df.sample(16901)['description'])\n",
    "topics_Full_Trending, corpus, dictionary = do_LDA(full_trending_lda_input)\n",
    "lda_display = visualize_LDA(True, corpus, dictionary)\n",
    "pyLDAvis.display(lda_display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## written by Aravind Patnam\n",
    "\n",
    "## this will take a long time to run!\n",
    "\n",
    "## visualize nontrending videos using pyLDAvis -> this might be only visible on nbviewer depending on your notebook viewing settings\n",
    "\n",
    "\n",
    "full_nontrending_lda_input = list(get_most_common_tags(full_nontrending_df).keys()) + list (full_nontrending_df['description'])\n",
    "topics_Full_Nontrending, corpus, dictionary = do_LDA(full_nontrending_lda_input)\n",
    "lda_display = visualize_LDA(True, corpus, dictionary)\n",
    "pyLDAvis.display(lda_display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## written by Aravind Patnam\n",
    "\n",
    "## this will take a long time to run!\n",
    "\n",
    "## visualize all videos using pyLDAvis -> this might be only visible on nbviewer depending on your notebook viewing settings\n",
    "\n",
    "allDfsDf_lda_input = list(get_most_common_tags(allDfsDf).keys()) + list (allDfsDf.sample(100000)['description'])\n",
    "topics_all_dfs, corpus, dictionary = do_LDA(allDfsDf_lda_input)\n",
    "lda_display = visualize_LDA(True, corpus, dictionary)\n",
    "pyLDAvis.display(lda_display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## written by Aravind Patnam\n",
    "## no need to run this as this has already been done for you\n",
    "## convert to csvs\n",
    "\n",
    "not_trending_us_df.to_csv('data/not_trending_us_df.csv')\n",
    "not_trending_ca_df.to_csv('data/not_trending_ca_df.csv')\n",
    "not_trending_de_df.to_csv('data/not_trending_de_df.csv')\n",
    "not_trending_fr_df.to_csv('data/not_trending_fr_df.csv')\n",
    "not_trending_gb_df.to_csv('data/not_trending_gb_df.csv')\n",
    "not_trending_in_df.to_csv('data/not_trending_in_df.csv')\n",
    "not_trending_jp_df.to_csv('data/not_trending_jp_df.csv')\n",
    "not_trending_kr_df.to_csv('data/not_trending_kr_df.csv')\n",
    "not_trending_mx_df.to_csv('data/not_trending_mx_df.csv')\n",
    "not_trending_ru_df.to_csv('data/not_trending_ru_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# written by Jeremy Tan \n",
    "# How long did a video stay trending?\n",
    "new = pd.DataFrame(full_trending_df_fill.groupby([full_trending_df_fill.index,'country']).count()['title'].sort_values(ascending=False)).reset_index()\n",
    "new.head(), new.tail()\n",
    "video_list,max_list = list(),list()\n",
    "country_list = full_trending_df.groupby(['country']).count().index\n",
    "\n",
    "bokeh.io.reset_output()\n",
    "bokeh.io.output_notebook()\n",
    "\n",
    "# gabs countries and grabs the times it appears in the dataframe\n",
    "# print(country_list)\n",
    "for c in country_list:\n",
    "    video_list.append(new[new['country']==c]['title'].value_counts().sort_index())\n",
    "    max_list.append(max(new[new['country']==c]['title'].value_counts().sort_index().index))\n",
    "# print(video_list) \n",
    "# print(max_list)\n",
    "\n",
    "# make line plots for the different countries (repeat another 9 times)\n",
    "first_fig = figure(x_axis_type='linear',\n",
    "             plot_height=300, plot_width=600,\n",
    "             title='How long did a video stay trending?',\n",
    "             x_axis_label='Conccurent Appearences (days)', y_axis_label=country_list[0],\n",
    "             toolbar_location=None)\n",
    "\n",
    "# plot for different countries (repeat another 9 times)\n",
    "first_fig.line(x=video_list[0].index, y=video_list[0], \n",
    "         color='gray', line_width=1)\n",
    "\n",
    "second_fig = figure(x_axis_type='linear',\n",
    "             plot_height=300, plot_width=600,\n",
    "             title='How long did a video stay trending?',\n",
    "             x_axis_label='Conccurent Appearences (days)', y_axis_label=country_list[1],\n",
    "             toolbar_location=None)\n",
    "\n",
    "second_fig.line(x=video_list[1].index, y=video_list[1], \n",
    "         color='gray', line_width=1)\n",
    "\n",
    "third_fig = figure(x_axis_type='linear',\n",
    "             plot_height=300, plot_width=600,\n",
    "             title='How long did a video stay trending?',\n",
    "             x_axis_label='Conccurent Appearences (days)', y_axis_label=country_list[2],\n",
    "             toolbar_location=None)\n",
    "\n",
    "third_fig.line(x=video_list[2].index, y=video_list[2], \n",
    "         color='gray', line_width=1)\n",
    "\n",
    "\n",
    "fourth_fig = figure(x_axis_type='linear',\n",
    "             plot_height=300, plot_width=600,\n",
    "             title='How long did a video stay trending?',\n",
    "             x_axis_label='Conccurent Appearences (days)', y_axis_label=country_list[3],\n",
    "             toolbar_location=None)\n",
    "\n",
    "fourth_fig.line(x=video_list[3].index, y=video_list[3], \n",
    "         color='gray', line_width=1)\n",
    "\n",
    "fifth_fig = figure(x_axis_type='linear',\n",
    "             plot_height=300, plot_width=600,\n",
    "             title='How long did a video stay trending?',\n",
    "             x_axis_label='Conccurent Appearences (days)', y_axis_label=country_list[4],\n",
    "             toolbar_location=None)\n",
    "\n",
    "fifth_fig.line(x=video_list[4].index, y=video_list[4], \n",
    "         color='gray', line_width=1)\n",
    "\n",
    "sixth_fig = figure(x_axis_type='linear',\n",
    "             plot_height=300, plot_width=600,\n",
    "             title='How long did a video stay trending?',\n",
    "             x_axis_label='Conccurent Appearences (days)', y_axis_label=country_list[5],\n",
    "             toolbar_location=None)\n",
    "\n",
    "sixth_fig.line(x=video_list[5].index, y=video_list[5], \n",
    "         color='gray', line_width=1)\n",
    "\n",
    "\n",
    "seventh_fig = figure(x_axis_type='linear',\n",
    "             plot_height=300, plot_width=600,\n",
    "             title='How long did a video stay trending?',\n",
    "             x_axis_label='Conccurent Appearences (days)', y_axis_label=country_list[6],\n",
    "             toolbar_location=None)\n",
    "\n",
    "seventh_fig.line(x=video_list[6].index, y=video_list[6], \n",
    "         color='gray', line_width=1)\n",
    "\n",
    "eight_fig = figure(x_axis_type='linear',\n",
    "             plot_height=300, plot_width=600,\n",
    "             title='How long did a video stay trending?',\n",
    "             x_axis_label='Conccurent Appearences (days)', y_axis_label=country_list[7],\n",
    "             toolbar_location=None)\n",
    "\n",
    "eight_fig.line(x=video_list[7].index, y=video_list[7], \n",
    "         color='gray', line_width=1)\n",
    "\n",
    "ninth_fig = figure(x_axis_type='linear',\n",
    "             plot_height=300, plot_width=600,\n",
    "             title='How long did a video stay trending?',\n",
    "             x_axis_label='Conccurent Appearences (days)', y_axis_label=country_list[8],\n",
    "             toolbar_location=None)\n",
    "\n",
    "ninth_fig.line(x=video_list[8].index, y=video_list[8], \n",
    "         color='gray', line_width=1)\n",
    "\n",
    "tenth_fig = figure(x_axis_type='linear',\n",
    "             plot_height=300, plot_width=600,\n",
    "             title='How long did a video stay trending?',\n",
    "             x_axis_label='Conccurent Appearences (days)', y_axis_label=country_list[9],\n",
    "             toolbar_location=None)\n",
    "\n",
    "tenth_fig.line(x=video_list[9].index, y=video_list[9], \n",
    "         color='gray', line_width=1)\n",
    "\n",
    "#Add panels for tabs \n",
    "first_panel = Panel(child=first_fig, title='Canada')\n",
    "second_panel = Panel(child=second_fig, title='Germany')\n",
    "third_panel = Panel(child=third_fig, title='France')\n",
    "fourth_panel = Panel(child=fourth_fig, title='Great Britan')\n",
    "fifth_panel = Panel(child=fifth_fig, title='India')\n",
    "sixth_panel = Panel(child=sixth_fig, title='Japan')\n",
    "seventh_panel = Panel(child=seventh_fig, title='Korea')\n",
    "eighth_panel = Panel(child=eight_fig, title='Mexico')\n",
    "ninth_panel = Panel(child=ninth_fig, title='Russia')\n",
    "tenth_panel = Panel(child=tenth_fig, title='United States')\n",
    "\n",
    "\n",
    "# Assign the panels to Tabs\n",
    "tabs = Tabs(tabs=[first_panel, second_panel, third_panel, fourth_panel, fifth_panel, sixth_panel, seventh_panel, eighth_panel, ninth_panel, tenth_panel])\n",
    "show(tabs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the visualization above, most trending videos only stay trending for two days or less. Some notable outliers are videos from Great Britan, which has videos that will stay trending for close to 30 days! For countries in the far East, videos only seem to stay tredning for one day so the turnover rate is pretty hight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Written by Jeremy Tan \n",
    "# Makes two new columns that count tilte length and description length\n",
    "full_trending_df['title_length'] = full_trending_df['title'].str.len()\n",
    "full_trending_df['description_length'] = full_trending_df['description'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Written by Jeremy Tan\n",
    "# What is the correlation between views, likes, disklies, and comment count \n",
    "full_trending_df = full_trending_df.rename(columns={'comment_count':'comments'})\n",
    "# make a scatter matrix to visualzie correlations\n",
    "fig = px.scatter_matrix(full_trending_df, dimensions=['views', 'likes', 'dislikes', 'comments', 'title_length', 'description_length'])\n",
    "fig.update_traces(opacity=0.5, showupperhalf=False)\n",
    "fig.update_layout(title = \"Correlations\", width = 900, height = 900)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen from the scatter matrix, there is a somewhat strong correlation between views and likes. Another correlation is between likes and comments, views and comments, and dislikes and comments (though somewhat weak!). Another interesting point is the length of the description and the length of the title seem to be tied together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the correlation between views, likes, disklies, and comment count?\n",
    "# Another interactive visualization \n",
    "corr = full_trending_df.loc[:, ['views', 'likes', 'dislikes', 'comments', 'description_length', 'title_length']].corr()\n",
    "fig2 = go.Figure(data=go.Heatmap(\n",
    "        z=corr.values,\n",
    "        x=corr.index,\n",
    "        y=corr.index,\n",
    "        colorscale=\"Blues\",\n",
    "        zmin=-1,\n",
    "        zmax=1\n",
    "))\n",
    "fig2.update_layout(title='Correlations between title, description, comments, dislikes, likes, and views')\n",
    "fig2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to visualzie the correlations. Here, I can see additional correlations of dislikes and views, dislikes and likes, and dislikes and comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Written by Jeremy Tan\n",
    "# What is the correlation between views, likes, disklies, and comment count in categories?\n",
    "categories = full_trending_df['category'].unique()\n",
    "interactions_corr_list = [full_trending_df[full_trending_df['category'] == cat].loc[:, ['views', 'likes', 'dislikes', 'comments', 'description_length', 'title_length']].corr() for cat in categories]\n",
    "fig3 = go.Figure()\n",
    "for idx, corr in enumerate(interactions_corr_list):\n",
    "    fig3.add_trace(\n",
    "        go.Heatmap(\n",
    "            z=corr.values,\n",
    "            x=corr.index,\n",
    "            y=corr.index,\n",
    "            colorscale=\"Blues\",\n",
    "            zmin=-1,\n",
    "            zmax=1,\n",
    "            visible=False)) \n",
    "\n",
    "# Add buttons\n",
    "fig3.update_layout(\n",
    "    updatemenus=[\n",
    "        go.layout.Updatemenu(\n",
    "            buttons=list([\n",
    "                dict(label=cat,\n",
    "                     method=\"update\",\n",
    "                     args=[{\"visible\": [False if sub_idx != idx else True for sub_idx, sub_cat in enumerate(categories)]},\n",
    "                           {\"title\": \"Correlation heatmap for category: \" + cat}])\n",
    "                for idx, cat in enumerate(categories)\n",
    "            ] ) \n",
    "        )\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Click a category you want to see the correlation of!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to visualize correlations but now based on categories. For the popular categories, the correlations I stated earlier hold true. However, if you were to go to a unpopular category like \"trailers,\" negative correlations appear in place of the previous, strong correlations. Most suprisingly, for categories that elict human emotion, such as \"Pet & Animals\" and \"Nonprofits & Activism\" there is a strong correlation between likes, views, comments, and dislikes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Written by Jeremy Tan\n",
    "# What category of videos trend the most in which countries?\n",
    "# What category of videos fail to hit trending in which countries?\n",
    "def trendingCategories(country, name):\n",
    "    cat = country['category'].value_counts().reset_index()\n",
    "    pop = px.bar(x=cat['category'], y=cat['index'], orientation='h')\n",
    "    pop.update_layout(title_text=\"Most successful categories of trend videos in \" + name, height=400)\n",
    "    pop.update_xaxes(title_text=\"Number of Videos\")\n",
    "    pop.update_yaxes(title_text=\"Categories\")\n",
    "    pop.show()\n",
    "    \n",
    "def nontrendingCategories(country, name):\n",
    "    cat = country['category'].value_counts().reset_index()\n",
    "    pop = px.bar(x=cat['category'], y=cat['index'], orientation='h')\n",
    "    pop.update_layout(title_text=\"Categories that fail to trend in \" + name, height=400)\n",
    "    pop.update_xaxes(title_text=\"Number of Videos\")\n",
    "    pop.update_yaxes(title_text=\"Categories\")\n",
    "    pop.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trendingCategories(US_trending_df, \"United States\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nontrendingCategories(not_trending_us_df, \"United States\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the two plot above, one can see \"Entertainment\" videos are the majority of videos made. They, however, have the highest chance to both fail and succeed. The pattern illuminates the more successful the category, the more videos that are being put out.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trendingCategories(CA_trending_df, \"Canada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nontrendingCategories(not_trending_ca_df, \"Canada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same pattern seems to appear as it did in the United States. However, \"People and Blogs\" and \"Music\" seem to fail more in this country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trendingCategories(DE_trending_df, \"Germany\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nontrendingCategories(not_trending_de_df, \"Germany\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same pattern seems to appear as it did in the United States. However, \"People and Blogs\" have more of a success rate but \"Films & Animation\" seem to fail more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trendingCategories(FR_trending_df, \"France\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nontrendingCategories(not_trending_fr_df, \"France\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same pattern occurs like in the Untied States. An interesting point is that \"Peoples & Blog\" have a higher probability of being trending. We can assume this is due to people valuing these type of videos more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trendingCategories(GB_trending_df, \"Great Britan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nontrendingCategories(not_trending_gb_df, \"Great Britan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same pattern occurs like in the Untied States. However, \"Music\" dominates this country with the majority of the videos being produced are of this category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trendingCategories(IN_trending_df, \"India\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nontrendingCategories(not_trending_in_df, \"India\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same pattern as the US. Nothing interesitng to note as it follows the pattern exactly where the more successful a category, the more likely the video will also fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trendingCategories(JP_trending_df, \"Japan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nontrendingCategories(not_trending_jp_df, \"Japan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar pattern as Germany, where \"People and Blogs\" videos will have a higher chance to be tredning and not fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trendingCategories(KR_trending_df, \"Korea\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nontrendingCategories(not_trending_kr_df, \"Korea\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same pattern as the US.Interesting enough, this country cares alot about \"News & Politics\" and have lots of video that hit trending (only second behind \"Entertainment\"). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trendingCategories(MX_trending_df, \"Mexico\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nontrendingCategories(not_trending_mx_df, \"Mexico\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same pattern as the US. Similar stats as the US too. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trendingCategories(RU_trending_df, \"Russia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nontrendingCategories(not_trending_ru_df, \"Russia\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same pattern as the US. However, like Germany and Japan, \"Peoples & Blogs\" have a high succes rate to reach trending and fail less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Written by Jeremy Tan\n",
    "# Which categories are the most popular? Do highest average amongst likes and views ----> shows which type of videos people enjoy the most\n",
    "# makes a box plot to visuzlaize distbution of likes and views \n",
    "def likes_to_categories(country, name, log):\n",
    "    country['likes_log'] = np.log(country['likes'] + 1)\n",
    "    country['views_log'] = np.log(country['views'] + 1)\n",
    "    country['dislikes_log'] = np.log(country['dislikes'] + 1)\n",
    "    country['comment_log'] = np.log(country['comment_count'] + 1)\n",
    "    if \"likes\" in log:\n",
    "        fig = px.box(country, y=log, x='category')\n",
    "        fig.update_layout(title_text=\"Like Distribuition by Category in \" + name, height=700)\n",
    "        fig.update_yaxes(title_text=\"Likes(log)\")\n",
    "        fig.show()\n",
    "    else:\n",
    "        fig = px.box(country, y=log, x='category')\n",
    "        fig.update_layout(title_text=\"View Distribuition by Category in \" + name, height=700)\n",
    "        fig.update_yaxes(title_text=\"Views(log)\")\n",
    "        fig.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likes_to_categories(US_trending_df, \"United States\", 'likes_log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likes_to_categories(US_trending_df, \"United States\", 'views_log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Judging from the boxplot, it seems people in the US enjoy videos categorized Music more the most. Gaming is the second most popualr. Then, Entertainment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likes_to_categories(CA_trending_df, \"Canada\", 'likes_log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likes_to_categories(CA_trending_df, \"Canada\", 'views_log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It isn't as clear, but there seems to be a tie between Movies and Music as some of the more popular videos in Canada. Comedy lags behind third. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likes_to_categories(DE_trending_df, \"Germany\", 'likes_log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likes_to_categories(DE_trending_df, \"Germany\", 'views_log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Germany, Music is the most popular category. Movies trail right behind, and Comedy right after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likes_to_categories(FR_trending_df, \"France\", 'likes_log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likes_to_categories(FR_trending_df, \"France\", 'views_log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In France, Music is the most popular category like Germany. However, Comedy and Entertainment trail right behind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likes_to_categories(GB_trending_df, \"Great Britan\", 'likes_log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likes_to_categories(GB_trending_df, \"Great Britan\", 'views_log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Music is the most popular category, while Nonprfits & Activism then Entertainment trail behind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likes_to_categories(IN_trending_df, \"India\", 'likes_log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likes_to_categories(IN_trending_df, \"India\", 'views_log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It isn't clear which videos come out on top, but based on likes, Pets & Animals, Gaming, and then Comedy are the top categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likes_to_categories(JP_trending_df, \"Japan\", 'likes_log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likes_to_categories(JP_trending_df, \"Japan\", 'views_log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funny enough, Science & Technology are the most popular videos follwoed by Music and then Comedy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likes_to_categories(KR_trending_df, \"Korea\", 'likes_log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likes_to_categories(KR_trending_df, \"Korea\", 'views_log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to Japan, Science & Technology is first as Musis is second. However, Sports is third. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likes_to_categories(MX_trending_df, \"Mexico\", 'likes_log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likes_to_categories(MX_trending_df, \"Mexico\", 'views_log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Music is first, Gaming is second, and Comedy is third."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likes_to_categories(RU_trending_df, \"Russia\", 'likes_log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likes_to_categories(RU_trending_df, \"Russia\", 'views_log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Music is first. Science and Technology is second. Comedy is third. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Written by Jeremy Tan\n",
    "# Which channels are the most successfucl at reaching trending?\n",
    "# Grabs most reoccuring videos in trending \n",
    "def videos_top(country, name):\n",
    "  \n",
    "    tmp = country.channel_title.value_counts()[:25]    \n",
    "   \n",
    "    pop = px.bar(x=tmp, y=tmp.index, orientation='h')\n",
    "    pop.update_layout(title_text=\"Top Channels\", height=700)\n",
    "    pop.update_xaxes(title_text=\"# times channel reached trending\")\n",
    "    pop.update_yaxes(title_text=\"channel name\")\n",
    "\n",
    "    pop.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_top(US_trending_df, \"United States\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_top(CA_trending_df, \"Canada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_top(DE_trending_df, \"Germany\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_top(FR_trending_df, \"France\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_top(GB_trending_df, \"Great Britan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_top(IN_trending_df, \"India\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_top(JP_trending_df, \"Japan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original scrapper did not properly encode characters correctly. Hence the weird symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_top(KR_trending_df, \"Korea\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original scrapper did not properly encode characters correctly. Hence the weird symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_top(MX_trending_df, \"Mexico\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_top(RU_trending_df, \"Russia\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original scrapper did not properly encode characters correctly. Hence the weird symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Written by Jeremy Tan\n",
    "# prep data to by adding like rate and spliitng publish time into a hour, min, and sec column\n",
    "full_trending_df['like_rate'] =  full_trending_df ['likes'] / full_trending_df['views'] * 100\n",
    "full_trending_df[['hour','min','sec']] = full_trending_df['publish_time'].astype(str).str.split(':', expand=True).astype(int)\n",
    "full_trending_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Written by Jeremy Tan\n",
    "# When a video gets published, what is the intial like rate that got them to trending?\n",
    "# Has two plots: one shows which hour a video is most commonly published and the other is a boxen plot that shows differnet quartiles of the like_rate + outliers\n",
    "\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=1, subplot_titles=(\"Hours Videos are Published\", \"Like Rate\")\n",
    ")\n",
    "\n",
    "\n",
    "s = full_trending_df['hour'].value_counts() \n",
    "new = pd.DataFrame({'Hour':s.index, 'Count':s.values}) \n",
    "\n",
    "fig.add_trace(go.Bar(x=new['Hour'], y=new['Count']), row=1, col=1)\n",
    "fig.add_trace(go.Box(x=full_trending_df[\"hour\"], y=full_trending_df[\"like_rate\"]), row=2, col=1)\n",
    "fig.update_xaxes(title_text=\"Hour\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"Hour\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"Count\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Like Rate(log)\", row=2, col=1)\n",
    "\n",
    "\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems most trending videos are published at 4 pm. However, more engagemnt in terms of like happens at 7pm. The graphs make the notebook lag, but hiding one of the traces helps!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_nontrending_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Written by Jeremy Tan \n",
    "# Grabs the videos that have the most views, likes, or dislikes\n",
    "# What videos have the most views, likes, and dislikes in the tredning dataset and nontrending dataset?\n",
    "def visualize_most(my_df, column): \n",
    "    # sort df by column\n",
    "    sorted_df = my_df.sort_values(column, ascending=False).iloc[:10]\n",
    "    # make a bar plot\n",
    "    pop = px.bar(x=sorted_df['title'], y=sorted_df[column])\n",
    "    pop.update_layout(title_text=\"Top Ten \" + column, height=700)\n",
    "    pop.update_yaxes(title_text=\"#\")\n",
    "    pop.update_xaxes(title_text=\"Video Title\")\n",
    "    pop.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_most(full_trending_df, \"views\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_most(full_nontrending_df, \"views\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_most(full_trending_df, \"likes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_most(full_nontrending_df, \"likes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_most(full_trending_df, \"dislikes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_most(full_nontrending_df, \"dislikes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Written by Jeremy Tan\n",
    "# Which country has the most active participation and engagemet?\n",
    "country_list = full_trending_df.groupby(['country']).count().index\n",
    "p = ['views', 'likes', 'dislikes', 'comments']\n",
    "measures = list()\n",
    "for i, typ in enumerate(p):\n",
    "    measure = list()\n",
    "    for c in country_list:\n",
    "        measure.append(full_trending_df[full_trending_df['country']==c][typ].agg('sum')/len(full_trending_df[full_trending_df['country']==c].index.unique()))\n",
    "    measures.append(measure)\n",
    "    \n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2, subplot_titles=(\"Views\", \"Likes\", \"Dislikes\", \"Comments\")\n",
    ")\n",
    "\n",
    "# Add traces\n",
    "fig.add_trace(go.Bar(x=list(country_list), y=measures[0]), row=1, col=1)\n",
    "fig.add_trace(go.Bar(x=list(country_list), y=measures[1]), row=1, col=2)\n",
    "fig.add_trace(go.Bar(x=list(country_list), y=measures[2]), row=2, col=1)\n",
    "fig.add_trace(go.Bar(x=list(country_list), y=measures[3]), row=2, col=2)\n",
    "fig.update_layout(title_text=\"Viewer Engagement\", height=700)\n",
    "\n",
    "fig.update_xaxes(title_text=\"countries\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"countries\", row=1, col=2)\n",
    "fig.update_xaxes(title_text=\"countries\",  row=2, col=1)\n",
    "fig.update_xaxes(title_text=\"countries\", row=2, col=2)\n",
    "\n",
    "fig.update_yaxes(title_text=\"num\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"num\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"num\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"num\", row=2, col=2)\n",
    "\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, Great Britan has the most active audience with the US seriously lagging behind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
