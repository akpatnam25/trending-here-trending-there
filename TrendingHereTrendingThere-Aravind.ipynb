{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/botocore/awsrequest.py:624: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  class HeadersDict(collections.MutableMapping):\n",
      "/usr/local/lib/python3.7/site-packages/gensim/corpora/dictionary.py:11: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Mapping, defaultdict\n",
      "/usr/local/lib/python3.7/site-packages/scipy/sparse/sparsetools.py:21: DeprecationWarning: `scipy.sparse.sparsetools` is deprecated!\n",
      "scipy.sparse.sparsetools is a private module for scipy.sparse, and should not be used.\n",
      "  _deprecated()\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.book import *\n",
    "from nltk.corpus import stopwords\n",
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "import random\n",
    "from nltk import classify\n",
    "from nltk import NaiveBayesClassifier\n",
    "from nltk.tokenize import word_tokenize\n",
    "import seaborn as sns\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "sns.set_context('notebook')\n",
    "import warnings\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "from nltk.corpus import stopwords\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### get all csv dataframes\n",
    "US_df = pd.read_csv('youtube-new/USvideos.csv') #USA\n",
    "CA_df = pd.read_csv('youtube-new/CAvideos.csv') #CANADA\n",
    "DE_df = pd.read_csv('youtube-new/DEvideos.csv') #GERMANY\n",
    "FR_df = pd.read_csv('youtube-new/FRvideos.csv') #FRANCE\n",
    "GB_df = pd.read_csv('youtube-new/GBvideos.csv') #GREAT BRITAIN\n",
    "IN_df = pd.read_csv('youtube-new/INvideos.csv') #INDIA\n",
    "\n",
    "# JP_df = pd.read_csv('youtube-new/JPvideos.csv', encoding=\"UTF-8\") #JAPAN\n",
    "\n",
    "# KR_df = pd.read_csv('youtube-new/KRvideos.csv') #SOUTH KOREA\n",
    "\n",
    "# MX_df = pd.read_csv('youtube-new/MXvideos.csv') #MEXICO\n",
    "\n",
    "# RU_df = pd.read_csv('youtube-new/RUvideos.csv') #RUSSIA\n",
    "\n",
    "\n",
    "full_df = pd.concat([US_df, CA_df, DE_df, FR_df, GB_df, IN_df])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'comedy': 305, 'Trailer': 272, 'animals': 262, 'how to': 257, 'funny': 253, 'food': 251, 'news': 247, 'basketball': 247, 'Pop': 240, 'music': 222, 'education': 219, 'science': 217, 'animation': 212, 'live': 209, 'official': 209, 'makeup': 203, 'Stephen Colbert': 194, 'vlog': 192, 'Jimmy Fallon': 191, 'Netflix Original Series': 183, 'Happening Now': 180, 'CNN': 179, 'highlights': 171, 'The': 170, 'Colbert': 167, 'Capitol': 165, 'tomscott': 159, 'Review': 154, 'world wrestling entertainment': 152, 'Seth Meyers': 152, 'plays': 150, 'fwf': 148, 'firstwefeast': 148, 'NBC': 148, 'good mythical morning': 147, 'cat': 145, 'TED Ed': 144, 'Records': 143, 'CH originals': 143, 'vox': 140, 'life noggin youtube': 139, 'espn live': 139, 'Saturday Night Live': 135, 'Hank': 135, 'Green': 135, 'hannah': 133, 'talk': 131, 'dude perfect stereotypes': 131, 'gmm': 130, 'Football': 129, 'screen junkies': 129, 'beauty': 128, 'gbs': 128, 'lag': 128, 'documentary': 128, 'television': 125, 'comics': 125, 'comic books': 125, 'Beauty': 123, 'SNL Season 43': 122, 'explain': 120, 'james charles': 118, 'Makeup': 118, '2018': 117, 'late': 117, 'ellen degeneres': 114, 'TED-Ed': 112, 'RCA Records Label': 112, 'fun': 112, 'BTS': 110, 'complex': 109, 'complex originals': 109, 'kimmel': 108, 'night': 108, 'fox sports': 108, 'fs1': 108, 'fox sports 1': 108, 'offense': 106, 'defense': 106, 'BANGTAN': 106, 'afc': 105, 'interview': 105, 'superwoman': 105, 'game': 104, \"simon's cat\": 103, 'Tutorial': 103, 'Graham Norton Show Official': 102, 'the': 101, 'BuzzFeedVideo': 101, 'DIY': 100, 'movie': 100, 'Jack Douglass': 100, 'YGS': 100, 'YGS 100': 100, 'TODAY Show': 99, 'TODAY': 99, 'Movie': 98, 'Machine': 98, 'reality': 97, 'simons cat': 97, 'refinery 29': 96, 'r29': 96, 'r29 video': 96, 'celebrity': 96, 'Film': 94, 'Late Show': 94, 'To': 92, 'will smith': 92, '방탄소년단': 91, 'Penatonix': 90, 'PTX': 90, 'PTXofficial': 90, 'itsalexclark': 90, 'alex clark': 90, 'do it yourself': 90, 'higa': 89, 'higatv': 89, 'nigahiga': 89, 'Island': 89, 'trailer': 88, 'play': 87, 'americanidol': 87, 'idol': 87, 'american idol': 87, 'Late Late Show': 86, 'BuzzFeed Video': 84, 'Big': 83, 'lizza': 83, 'the dodo': 83, 'bbc news': 82, 'screenjunkies': 81, 'Yolanda Gampp': 81, 'video': 81, 'R&B': 81, 'laurDIY': 81, 'Megatoad': 81, 'Competitive Eating': 81, '빅히트': 80, 'acoustic': 78, 'william osman': 77, 'crappy science': 77, 'Movie Trailers': 77, 'team super': 77, 'charles': 75, 'will': 75, 'The Late Late Show': 74, 'lizzza': 74, 'lizzzak': 74, 'tutorial': 74, 'YouTube': 74, 'nerd': 74, 'geek': 74, 'BYU tv': 74, 'BYUtelevision': 74, 'Studio C': 74, 'TED Education': 73, 'music video': 73, 'Marvel': 73, 'pets': 73, 'jimmy kimmel': 72, 'Animation': 72, 'review': 71, 'adventurous': 71, 'anvil vs': 71, 'kids': 70, 'the ellen show': 70, 'guava juice youtube': 70, 'black panther': 70, 'marvel': 70, 'super': 69, 'us news': 69, 'Rob Czar': 69, 'You': 69, 'world of wonder productions': 69, 'EMI': 68, 'jimmy kimmel live': 68, 'simon': 68, 'furze': 68, 'Me': 67, 'lizzzavine': 67, 'play nintendo': 67, 'slow': 67, 'Comedy': 67, 'Columbia': 67, '4K': 67, 'cooking': 66, 'animations': 66, 'Ted Education': 66, 'to': 65, 'reaction': 65, 'stocking': 65, '3D Animation': 65, 'c&hshorts': 64, 'c&h shorts': 64, 'billboard channel': 64, 'tv': 64, 'cbs': 64, 'Movies': 64, 'New': 64, 'Diplo': 64, 'youtube': 63, 'Full Frontal': 63, 'anwar': 63, 'tyleroakley': 63, 'youtuber': 63, 'threadbanger': 63, 'Slow Motion': 63, 'nba': 62, 'nerdwriter': 62, 'giant anvil vs': 62, 'lele': 61, 'ed sheeran': 61, 'stars': 61, 'colinfurze': 61, 'trevor noah': 61, 'britains got talent': 61, 'Celebrity': 60, 'Fvid': 60, 'Kevin Hart What the Fit': 60, 'Teded': 60, 'Of': 59, 'makeup tutorial': 59, 'Hollywood': 59, 'Home': 59, 'no excuses': 59, 'reviews': 58, 'power1051': 58, 'space': 58, 'diy': 57, 'camila': 57, '2017': 57, 'BBC': 57, 'howto': 57, 'Cats': 57, 'Cat Videos': 57, 'marbles': 57, 'mourey': 57, 'Virgin': 57, 'Money': 57, 'fortnite': 57, 'rudy': 56, 'mancuso': 56, 'pons': 56, 'Science': 56, 'Corinne Leigh': 56, 'christmas': 56, 'Dancing': 56, 'rug': 56, 'bybel': 56, 'Disney': 55, 'Gordon Ramsay': 55, 'amazon prime': 55, 'Delicate': 55, 'What': 55, 'mo': 54, 'motion': 54, 'BuzzFeed Tasty': 54, 'nat geo': 54, 'natgeo': 54, 'Disruptor Records/Columbia': 54, 'Anywhere': 54, 'dinos': 54, 'paleo': 54, 'paleontology': 54, 'Tasty': 54, 'nail art': 53, 'nail tutorial': 53, 'recipe': 53, 'SZA': 53, 'Wonderland': 53, 'aarons': 53, 'entertainment': 53, 'fort': 53, 'Japanese': 52, 'style': 52, 'cute': 52, 'Lip Sync Battle': 52, 'My': 52, 'Official': 51, 'Sports': 51, 'nerdy': 51, 'geeky': 51, 'Smith': 51, 'jaidenanimation': 51, 'Deadpool': 50, 'Ramsay': 50, 'Ramsey': 50, 'breaking news': 50, 'Out': 50, 'prince harry': 50, 'Jun': 50, 'Rachel': 50, 'vsauce2': 50, 'jibawi': 49, 'redbull': 49, 'action sports': 49, 'TED': 49, 'cats': 48, 'indie': 48, 'Video': 48, 'zoe': 48, 'zoella': 48, 'News': 48, 'fashion': 48, 'charlie': 48, 'Khalid': 48, 'Minute Earth': 48, 'MinutePhysics': 48, 'rugfaze': 48, 'hart': 47, 'huang': 47, 'andrew huang': 47, 'akana': 47, 'ana': 47, 'annaakana': 47, 'The Last Jedi': 47, 'MCA': 47, 'No': 47, 'br': 47, 'For': 46, 'pie': 46, 'grace helbig': 46, 'Amazon': 46, 'hanna hart': 46, 'a24 films': 46, 'a24 trailers': 46, 'funny vines': 46, 'funny videos': 46, 'pop culture': 46, 'VH+1': 46, 'women': 45, 'paul': 45, 'Pokemon': 45, 'Vox.com': 45, 'tour': 45, 'iphone': 44, 'you suck at cooking': 44, 'the hollywood reporter': 44, 'logan': 44, 'padilla': 44, 'dance': 44, 'adventure': 44, 'TEDEd': 44, 'Hawaii': 44, 'lip dub': 44, 'Love': 43, 'CBS This Morning': 43, 'the voice nbc': 43, 'Young': 43, 'olympics': 43, 'Marcus and Lucas': 43, 'Dobre': 43, 'Grande': 43, 'Tears': 43, 'Left': 43, 'makeup artist': 43, 'mua': 43, 'Rap': 42, 'sugg': 42, 'first take': 42, 'Music': 42, 'politics': 42, 'daytime': 42, 'talk show': 42, 'Rescue': 42, 'Xbox One': 42, 'RT': 42, 'Janelle': 42, 'asmr': 42, 'Scott': 42, 'gaming': 41, 'TBS Network': 41, 'TBS Shows': 41, 'vlogger': 41, 'Universal': 41, 'super bowl': 41, 'Back': 41, 'queen': 41, 'cod': 41, 'activision': 41, 'Boston Dynamics': 40, 'gus': 40, 'cake': 40, 'dancing': 40, 'dwts': 40, 'Disney Pixar': 40, 'Pixar Movie': 40, 'Dragons': 40, 'Gadgets': 40, 'saturday night live': 40, 'jaclyn hill': 40, 'TV': 40, 'Urban': 40, 'Rita': 40, 'Ora': 40, 'Musgraves': 40, 'Clash Royale Game': 40, 'TI': 39, 'Tech': 39, 'pop': 39, 'shows': 39, 'day': 39, 'Jonas': 39, 'Kylie': 39, 'star wars': 39, 'awards': 39, 'family games': 39, 'Sport': 39, 'Recorded': 39, 'NY Times': 38, 'NYT': 38, 'degeneres': 38, 'The CW Network': 38, 'jennxpenn2': 38, 'jennxpenngames': 38, 'High': 38, 'Youth': 38, 'how to make vanilla cupcakes': 37, 'ballinger': 37, 'Mamma Mia 2': 37, 'Mamma Mia Movie': 37, 'boy': 37, 'breaking': 37, 'binging with babish': 37, 'with': 36, 'pbs': 36, 'Paul': 36, 'BuzzFeed': 36, 'elle magazine': 36, 'elle video': 36, 'infinity war': 36, 'Rock': 36, 'PS4': 36, 'puth': 36, 'videogamedunkey': 36, 'Puth': 36, 'thegabbieshow': 36, 'mendes': 36, 'yiay': 35, 'cutiepie': 35, 'cutiepiemarzia': 35, 'make up': 35, 'what is': 35, 'brad': 35, 'Entertainment': 35, 'Olympics': 35, 'Bay': 35, 'vsauce3': 35, 'Because Science': 35, 'Kyle Hill': 35, 'itsbl0ndie': 35, 'madelainepetsch': 34, 'martina': 34, 'pizza': 34, 'news channel': 34, 'Meaning of Life': 34, 'buzzfeedvideo': 34, 'movies': 34, 'Your Song': 34, 'han solo': 34, 'Formula 1': 34, 'demi lovato': 34, 'good mythical more': 34, 'BuzzFeedBlue': 34, 'BuzzFeed Blue': 34, 'In My Blood': 34, 'Universal-Island': 34, 'kijiji': 34, 'homemade': 33, 'video updates': 33, 'candy': 33, 'Donald Trump': 33, '...\\n rhett and link': 33, 'rap genius': 33, 'verified': 33, 'Live': 33, 'Nerf Battle Universe': 33, '1million dance studio': 33, 'snl': 33, 'calvin harris one kiss': 33, '2018 nba playoffs': 33, 'mlg highlights': 33, 'PC': 33, 'House': 32, 'thomas sanders vine': 32, 'Miss': 32, 'cover': 32, 'ConnorFranta': 32, 'reacting': 32, 'The View': 32, 'computer': 32, 'royal wedding': 32, 'sorry': 32, 'girls': 32, 'thesorrygirls': 32, 'trash': 32, 'Pray': 32, 'As/Is': 32, 'Impulse Series': 32, 'Drama': 32, 'disney': 31, 'Real Time': 31, 'lgbt': 31, 'lindsay': 31, 'violin': 31, 'dubstep': 31, 'lifestyle': 31, 'ap': 31, 'home': 31, 'Payne': 31, 'Mad Money': 31, 'Squawk Box': 31, 'Power Lunch': 31, 'Hero4': 31, 'Hero5': 31, 'Hero Camera': 31, 'song': 31, 'jackson': 31, 'TEDTalks': 31, 'kids games': 31, 'solo': 31, 'biale': 31, 'Shane Dawson': 31, 'all you can eat': 31, 'Shows”': 31, '”truTV': 31, 'Series”': 31, 'IMYWIW18': 31, 'Mendes': 31, 'BBC Worldwide': 30, 'Nature': 30, 'celeb style': 30, 'design': 30, 'Your': 30, 'art': 30, 'time': 30, 'colleen': 30, 'espn first': 30, 'vacuum': 30, 'google': 30, 'Epic Games': 30, 'iphone x': 29, 'grace': 29, 'Hip Hop': 29, 'cartoon': 29, 'Beyonce': 29, 'breakfast': 29, 'hot topics': 29, 'the last jedi': 29, 'Lisa Elridge': 29, 'Lisa Eldrige': 29, 'of': 29, 'برشلونة،': 29, 'Fútbol': 29, 'FUTBOL': 29, 'Michaels': 29, 'arts and entertainment': 29, 'Coming': 29, 'New Movie': 29, 'Venom Movie': 29, 'Venom (2018)': 29, 'the wendy williams show': 29, 'discovery': 29, 'Panic! At The Disco': 29, 'Sivan': 29, 'Jay': 29, 'Chou': 29, '周董': 29, '周杰伦': 29, '周傑倫': 29, '杰威尔': 29, '周周': 29, 'setup': 29, 'smith': 28, 'dog': 28, 'taylor dean': 28, 'kpop': 28, 'Dogs': 28, '조쉬': 28, '올리': 28, 'Awards': 28, 'unboxing': 28, 'Feature': 28, 'cinemasins': 28, '222': 28, 'drone': 28, '...\\n Jacksfilms': 28, 'quick makeup': 28, 'Alone': 28, 'lut': 28, 'vsauce': 28, 'Bloom': 28, 'horse': 28, 'cobra kai new season': 28, 'Mayer': 28, 'Light': 28, 'Snack': 28, '13 reasons why': 28, 'back to you': 28, 'phage': 28, 'endolysin': 28, 'bohemian rhapsody': 28, 'Next': 27, 'Stapleton': 27, 'Gomez': 27, 'Christmas': 27, 'Gaga': 27, 'every blank ever': 27, 'Want': 27, 'elleofthemills': 27, 'Song': 27, 'In': 27, 'giant': 27, 'Swift': 27, 'WWE': 27, 'Formula One': 27, '10 minute makeup': 27, 'Broadway': 27, 'Business News': 27, 'scheduling': 27, 'mol': 27, 'vat19 nanodots': 27, 'vat19 infinity gauntle...\\n ryan': 27, 'rice': 27, 'one million': 27, 'larrabee': 27, 'prototype': 27, 'gpu': 27, \"Don't Go Breaking My Heart\": 27, 'we': 27, 'bought': 27, 'Most': 27, 'CBS News': 26, '...\\n BuzzFeed': 26, 'mental floss': 26, 'Julie Chen': 26, 'Aisha Tyler': 26, 'how-to': 26, 'liza koshy': 26, 'engineering': 26, 'seth meyers': 26, 'chef laura vitale': 26, 'drawing': 26, 'alissa ashley makeup': 26, 'gustoonz': 26, 'the bachelorette': 26, 'family': 26, 'Nick': 26, 'how': 26, 'British': 26, 'Fun': 26, 'patrick star': 26, 'Wait': 26, 'DaisyMarquez': 26, 'Daisy Marques': 26, 'HIPHOP': 26, '랩몬스터': 26, 'MerrellTwins': 26, 'Roman': 26, 'Atwood': 26, 'Us': 26, 'charlie puth': 26, 'National Hockey League': 26, 'Hockey': 26, 'ufc': 26, 'new song': 26, '방탄': 26, 'harmonizers': 26, 'lauren': 26, 'ally': 26, 'built for science': 25, 'DJ Walkzz': 25, 'K-391': 25, 'bass': 25, 'electrical': 25, 'electroboom': 25, '...\\n nba': 25, 'skate': 25, '레드벨벳': 25, 'marriage': 25, 'sings': 25, 'mirandasings08': 25, 'beautiful': 25, 'sketch': 25, 'All': 25, 'Animals': 25, 'Armie Hammer': 25, 'engagement': 25, 'logan paul': 25, 'Sci-Fi': 25, 'easy': 25, 'robot': 25, 'graceffa': 25, 'Day': 25, 'associated press': 25, 'ap online': 25, 'burger': 25, 'horses': 25, 'mercedes': 25, 'Walt Disney Animation Studios': 25, 'alain de botton': 25, '...\\n charlie puth': 25, 'wild': 25, '...\\n basics with babish': 25, 'This Is America': 25, 'Omar gosh': 25, 'Stan Van Gundy': 25, 'buzzfeed michelle': 25, 'organize': 25, 'Sia': 25, 'Labrinth': 25, 'LSD': 25, 'coconut': 25, 'laura lee': 25, 'mm7games': 25, 'random': 25, 'Nicki Minaj Queen': 25, 'daddy yankee youtube': 25, 'Puppy Interview': 25, 'jlo': 25, 'jennifer lopez live': 25, 'king': 24, 'On': 24, 'Polydor': 24, 'animated': 24, 'new': 24, '73 questions': 24, '...\\n tom scott': 24, 'usatsyn': 24, 'shane': 24, '...\\n great big story': 24, '...\\n jimmy': 24, 'TMZ': 24, 'blonde': 24, 'Star Wars Show': 24, 'graceinabox': 24, 'chocolate': 24, 'fried': 24, 'actress': 24, 'dogs': 24, 'kitty': 24, 'apple': 24, 'my': 24, 'desi perkins': 24, 'the perkins': 24, 'antscanada': 24, 'mikey bustos': 24, 'billwurtz': 24, 'country': 24, 'Manheim': 24, 'Meg': 24, 'Donnelly': 24, 'Someday': 24, 'Fahrenheit 451': 24, 'Guy Montag': 24, 'new zealand': 24, \"harper's bazaar\": 24, 'harpers bazaar': 24, 'marzipans': 24, 'proposal': 24, 'jumping': 24, 'mexican jumping beans': 24, 'creativity': 24, 'demo': 24, 'Five': 24, 'thesis film': 24, 'dragonfoxgirl': 24, 'heartfelt wedding video': 24, 'cheap': 23, 'chef': 23, 'björk': 23, 'channel': 23, '...\\n shorts': 23, 'cookies': 23, 'buzzfeed video': 23, '...\\n The Tonight Show': 23, 'the view': 23, 'etonline': 23, '...\\n latest News': 23, 'Metal': 23, 'Rey': 23, 'Twins': 23, 'davide biale': 23, 'bts': 23, 'dodie': 23, 'new york': 23, 'chris smoove': 23, 'Def': 23, 'Jam': 23, 'vacuum chamber': 23, 'fish': 23, 'online': 23, 'YEEZY': 23, 'Kanye': 23, 'Charlamagne': 23, 'olivia jade vlogs': 23, 'sogallant': 23, 'new music': 23, 'soul': 23, 'r&b': 23, 'worth it': 23, 'kebab buffet': 23, 'buffet': 23, '예능': 23, '일요예능': 23, '일밤': 23, '복면': 23, '가왕': 23, '복면가왕': 23, '김성주': 23, 'D...\\n Rooster Teeth': 23, 'Lil Pump': 23, 'juanpa': 23, 'Other': 22, 'Cooking': 22, '...\\n The TODAY Show': 22, 'real life lore maps': 22, \"how it's made\": 22, '...\\n SNL': 22, 'lyric video': 22, 'JOLLY': 22, 'jolly': 22, '졸리': 22, '조시': 22, 'Do it yourself': 22, 'kayley melissa': 22, 'VICE News': 22, 'desserts': 22, 'babish': 22, 'every': 22, 'Same': 22, 'Real Madrid': 22, 'trevor james': 22, 'food ranger': 22, 'smiths': 22, 'Nightly News': 22, 'bleecker street media': 22, 'vsause': 22, 'camila full album': 22, 'brad leone': 22, 'transformation': 22, 'theepatrickstarrr': 22, 'rexha': 22, 'Weeknd': 22, \"Don't\": 22, 'heyitsfei': 22, 'boyfriend': 22, 'north korea': 22, 'helbig': 22, 'gracehelbig': 22, 'arctic': 22, 'egg': 22, 'as is': 22, 'cat handling': 22, 'cat safety': 22, 'Céline Dion': 22, 'films': 22, 'Hunger': 22, 'original iphone': 22, 'iphone unboxing': 22, 'nintendo labo': 22, 't:Original': 22, 'Supercell': 22, 'Total Bellas': 22, 'wrestler': 22, 'lip sync': 22, '435': 22, 'nice toenails': 22, '...\\n life noggin': 21, 'challenge': 21, 'nfl': 21, 'kim kardashian west': 21, 'pbs digital studios': 21, '...\\n Netflix': 21, 'emotional': 21, '...\\n beauty': 21, 'Last': 21, 'last jedi': 21, 'film': 21, 'Fallen Kingdom': 21, '...\\n DIY': 21, 'taste test': 21, 'bad lip reading': 21, 'the voice': 21, 'Madrid': 21, 'vice': 21, 'cube': 21, 'K-pop': 21, 'US News': 21, 'Underwood': 21, 'Champion': 21, 'best': 21, 'Super Bowl': 21, 'meat': 21, 'total': 21, 'request': 21, 'mtv': 21, 'diy hacks': 21, 'Gospel/Christian': 21, 'basic': 21, 'prom': 21, 'scuba diving': 21, 'BuzzFeed Celeb': 21, 'cheap vs expensive': 21, 'gibi': 21, 'scratch': 21, 'scratching': 21, 'objects': 21, 'every museum ever': 21, 'Cry Pretty': 21, 'toronja': 21, 'grapefruit': 21, 'peel': 21, 'pith': 21, 'celebrity homes': 21, 'open door': 21, 'colour blind': 21, 'New Light': 21, 'Music Video': 21, 'nordvpn': 21, 'cactus': 21, 'livestock': 21, 'national geographic': 21, 'testedcom': 20, 'Featured': 20, 'Tesla': 20, 'school': 20, 'in': 20, 'travel': 20, 'Sony Pictures': 20, 'Movie Review': 20, 'MTV': 20, 'American Idol': 20, 'Queen': 20, 'karlie kloss': 20, 'john green': 20, '...\\n james': 20, 'humor': 20, 'book': 20, 'Munchies': 20, 'Munchiestv': 20, 'elle mills': 20, 'boldly': 20, 'mlg': 20, 'OMFGItsJackAndDean': 20, 'fire': 20, 'HBO': 20, 'model s': 20, 'model x': 20, 'ifc': 20, 'hollywood': 20, 'games': 20, 'Switch': 20, 'gameplay': 20, 'anthem': 20, \"Tia Mowry's Quick Fix\": 20, 'ration': 20, 'army': 20, 'commercial': 20, 'If': 20, 'grammy awards': 20, 'Avengers': 20, 'Summer': 20, 'simple': 20, 'rick and morty': 20, 'aluminum foil': 20, 'Natti Natasha': 20, 'Natti Natasha Music': 20, 'Somebody': 20, 'nickelodeon': 20, 'jess': 20, 'glynne': 20, \"i'll be there\": 20, 'Familiar': 20, 'black panther costume': 20, 's:Entertainment...\\n BYUtv': 20, 'ca...\\n Lucas and Marcus': 20, 'season 14 finale': 20, 'its': 20, 'NickyJamPR': 20, 'latin': 20, 'Banks': 20, 'Anna': 20, 'Wintour': 20, 'eOne': 20, 'The Sims 4 Trailer': 20, 'fortnite pc': 20, 'shawn mendes': 20, 'Taylor Swift': 20, '10': 20, 'Hayes': 19, 'cheese': 19, 'Horan': 19, 'life': 19, 'beauty secrets': 19, 'Steinfeld': 19, '1theK': 19, '원더케이': 19, 'loen': 19, '로엔': 19}\n"
     ]
    }
   ],
   "source": [
    "###get most common tags\n",
    "def get_most_common_tags(country_df):\n",
    "    tags = country_df['tags'].to_string(index=False, header=False)\n",
    "    split_tags = [i.replace('\"', '') for i in tags.split(\"|\")]\n",
    "    stop_words = stopwords.words('english')\n",
    "    filtered_tags = [word for word in split_tags if word not in stop_words]\n",
    "    fdist = FreqDist(split_tags)\n",
    "    most_popular_tags = fdist.most_common(1000)\n",
    "    return fdist, dict(most_popular_tags)\n",
    "\n",
    "fd1, us_most_common_tags = get_most_common_tags(US_df)\n",
    "f2, ca_most_common_tags = get_most_common_tags(CA_df)\n",
    "fd3, de_most_common_tags = get_most_common_tags(DE_df)\n",
    "fd4, fr_most_common_tags = get_most_common_tags(FR_df)\n",
    "fd5, gb_most_common_tags = get_most_common_tags(GB_df)\n",
    "fd6, in_most_common_tags = get_most_common_tags(IN_df)\n",
    "\n",
    "print (us_most_common_tags)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nword cloud \\nbar graph \\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####do visualization for most popular tags\n",
    "'''\n",
    "word cloud \n",
    "bar graph \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## do sentiment analysis on each of the tags\n",
    "positive_words_df = pd.read_fwf('positivewords.txt')\n",
    "negative_words_df = pd.read_fwf('negativewords.txt')\n",
    "\n",
    "def extract_features(words):\n",
    "    return dict([(word, True) for word in words.split()])\n",
    "\n",
    "def build_sentiment_analysis_model():\n",
    "    positive_words = positive_words_df['positivewords'].values.tolist()\n",
    "    negative_words = negative_words_df['negativewords'].values.tolist()\n",
    "    pos_feats = [(extract_features(f), 'positive') for f in positive_words ]\n",
    "    neg_feats = [(extract_features(f), 'negative') for f in negative_words ]\n",
    "    dataset = pos_feats + neg_feats\n",
    "    random.shuffle(dataset)\n",
    "    cutoff = int(0.80 * len(dataset))\n",
    "    train_data = dataset[:cutoff]\n",
    "    test_data = dataset[cutoff:]\n",
    "\n",
    "    classifier = NaiveBayesClassifier.train(train_data)\n",
    "    # print(\"Accuracy is:\", classify.accuracy(classifier, test_data))\n",
    "    # print(classifier.show_most_informative_features(10))\n",
    "    return classifier\n",
    "\n",
    "def execute_model(tags):\n",
    "    classifications = {}\n",
    "    classifier = build_sentiment_analysis_model()\n",
    "    classifier.show_most_informative_features(5)\n",
    "    for tag in tags:\n",
    "        classified = classifier.classify(extract_features(tag))\n",
    "        classifications[tag] = classified\n",
    "    return classifications\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "             enviousness = True           positi : negati =      2.4 : 1.0\n",
      "                 coolest = None           negati : positi =      1.0 : 1.0\n",
      "               righteous = None           negati : positi =      1.0 : 1.0\n",
      "                  snazzy = None           negati : positi =      1.0 : 1.0\n",
      "                 happily = None           negati : positi =      1.0 : 1.0\n",
      "Most Informative Features\n",
      "                 coolest = None           negati : positi =      1.0 : 1.0\n",
      "               righteous = None           negati : positi =      1.0 : 1.0\n",
      "                  snazzy = None           negati : positi =      1.0 : 1.0\n",
      "                 happily = None           negati : positi =      1.0 : 1.0\n",
      "                  firmer = None           negati : positi =      1.0 : 1.0\n",
      "Most Informative Features\n",
      "             enviousness = True           positi : negati =      2.4 : 1.0\n",
      "                 coolest = None           negati : positi =      1.0 : 1.0\n",
      "               righteous = None           negati : positi =      1.0 : 1.0\n",
      "                  snazzy = None           negati : positi =      1.0 : 1.0\n",
      "                 happily = None           negati : positi =      1.0 : 1.0\n",
      "Most Informative Features\n",
      "                 envious = True           positi : negati =      2.4 : 1.0\n",
      "             enviousness = True           positi : negati =      2.4 : 1.0\n",
      "               enviously = True           positi : negati =      2.4 : 1.0\n",
      "                 coolest = None           negati : positi =      1.0 : 1.0\n",
      "               righteous = None           negati : positi =      1.0 : 1.0\n",
      "Most Informative Features\n",
      "                 envious = True           positi : negati =      2.4 : 1.0\n",
      "               enviously = True           positi : negati =      2.4 : 1.0\n",
      "               righteous = None           negati : positi =      1.0 : 1.0\n",
      "                  snazzy = None           negati : positi =      1.0 : 1.0\n",
      "                 happily = None           negati : positi =      1.0 : 1.0\n",
      "Most Informative Features\n",
      "               enviously = True           positi : negati =      2.4 : 1.0\n",
      "                 coolest = None           negati : positi =      1.0 : 1.0\n",
      "               righteous = None           negati : positi =      1.0 : 1.0\n",
      "                  snazzy = None           negati : positi =      1.0 : 1.0\n",
      "                 happily = None           negati : positi =      1.0 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifications_us = execute_model(list(us_most_common_tags.keys()))\n",
    "classifications_ca = execute_model(list(ca_most_common_tags.keys()))\n",
    "classifications_de = execute_model(list(de_most_common_tags.keys()))\n",
    "classifications_fr = execute_model(list(fr_most_common_tags.keys()))\n",
    "classifications_gb = execute_model(list(gb_most_common_tags.keys()))\n",
    "classifications_in = execute_model(list(in_most_common_tags.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5% of the tags found in the trending youtube videos for country USA were positive in nature, while 97.5% were negative in nature.\n",
      "\n",
      "3.1% of the tags found in the trending youtube videos for country Canada were positive in nature, while 96.89999999999999% were negative in nature.\n",
      "\n",
      "1.0999999999999999% of the tags found in the trending youtube videos for country Denmakr were positive in nature, while 98.9% were negative in nature.\n",
      "\n",
      "1.3% of the tags found in the trending youtube videos for country France were positive in nature, while 98.7% were negative in nature.\n",
      "\n",
      "1.3% of the tags found in the trending youtube videos for country Great Britain were positive in nature, while 98.7% were negative in nature.\n",
      "\n",
      "1.9% of the tags found in the trending youtube videos for country India were positive in nature, while 98.1% were negative in nature.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_sentiment_stats(classification, country):\n",
    "    sentiments = list(classification.values())\n",
    "    sentiments_df = pd.DataFrame(sentiments, columns=['Sentiment'])\n",
    "    negatives = len(sentiments_df[sentiments_df['Sentiment'] =='negative'])\n",
    "    positives = len(sentiments_df[sentiments_df['Sentiment'] =='positive'])\n",
    "    total_len = len(sentiments_df)\n",
    "    percentage_of_negative = negatives / total_len * 100\n",
    "    percentage_of_positive = positives / total_len * 100\n",
    "    print (\"{}% of the tags found in the trending youtube videos for country {} were positive in nature, while {}% were negative in nature.\\n\".format(percentage_of_positive, country, percentage_of_negative))\n",
    "    \n",
    "get_sentiment_stats(classifications_us, \"USA\")\n",
    "get_sentiment_stats(classifications_ca, \"Canada\")\n",
    "get_sentiment_stats(classifications_de, \"Denmark\")\n",
    "get_sentiment_stats(classifications_fr, \"France\")\n",
    "get_sentiment_stats(classifications_gb, \"Great Britain\")\n",
    "get_sentiment_stats(classifications_in, \"India\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## do visualization of sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_LDA_model(country_tags):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "    # create English stop words list\n",
    "    stop_words = stopwords.words('english')\n",
    "\n",
    "    # Create p_stemmer of class PorterStemmer\n",
    "    p_stemmer = PorterStemmer()\n",
    "    # compile sample documents into a list\n",
    "    tags = list(country_tags.keys())\n",
    "\n",
    "    # list for tokenized documents in loop\n",
    "    texts = []\n",
    "\n",
    "    # loop through document list\n",
    "    for i in tags:\n",
    "\n",
    "        # clean and tokenize document string\n",
    "        raw = i.lower()\n",
    "        tokens = tokenizer.tokenize(raw)\n",
    "\n",
    "        # remove stop words from tokens\n",
    "        stopped_tokens = [i for i in tokens if not i in stop_words]\n",
    "\n",
    "        # stem tokens\n",
    "        stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "\n",
    "        # add tokens to list\n",
    "        texts.append(stemmed_tokens)\n",
    "\n",
    "    # turn our tokenized documents into a id <-> term dictionary\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "    # convert tokenized documents into a document-term matrix\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "    # generate LDA model\n",
    "    ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=3, id2word = dictionary, passes=100)\n",
    "    print(ldamodel.print_topics(num_topics=3, num_words=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.017*\"news\" + 0.010*\"video\" + 0.010*\"makeup\" + 0.009*\"music\" + 0.007*\"scienc\" + 0.007*\"beauti\" + 0.006*\"last\" + 0.006*\"iphon\" + 0.006*\"tutori\" + 0.006*\"pop\"'), (1, '0.012*\"anim\" + 0.007*\"sport\" + 0.007*\"film\" + 0.007*\"entertain\" + 0.006*\"one\" + 0.006*\"super\" + 0.006*\"puth\" + 0.006*\"diy\" + 0.006*\"review\" + 0.006*\"lip\"'), (2, '0.017*\"show\" + 0.012*\"movi\" + 0.011*\"new\" + 0.010*\"buzzfe\" + 0.010*\"cat\" + 0.009*\"live\" + 0.009*\"game\" + 0.008*\"late\" + 0.008*\"youtub\" + 0.008*\"video\"')]\n",
      "-----------------------------------------------------\n",
      "[(0, '0.025*\"news\" + 0.007*\"buzzfe\" + 0.007*\"ab\" + 0.006*\"cbn\" + 0.006*\"onlin\" + 0.006*\"jame\" + 0.006*\"laugh\" + 0.006*\"fail\" + 0.006*\"dr\" + 0.006*\"footbal\"'), (1, '0.017*\"tv\" + 0.017*\"show\" + 0.013*\"live\" + 0.012*\"nba\" + 0.011*\"highlight\" + 0.009*\"late\" + 0.008*\"trump\" + 0.008*\"anim\" + 0.007*\"world\" + 0.007*\"react\"'), (2, '0.012*\"video\" + 0.010*\"movi\" + 0.008*\"game\" + 0.008*\"fox\" + 0.008*\"2018\" + 0.007*\"sport\" + 0.007*\"funni\" + 0.007*\"youtub\" + 0.006*\"رمضان\" + 0.006*\"ted\"')]\n",
      "-----------------------------------------------------\n",
      "[(0, '0.011*\"deutsch\" + 0.010*\"die\" + 0.010*\"late\" + 0.007*\"music\" + 0.006*\"hop\" + 0.006*\"hip\" + 0.006*\"funni\" + 0.006*\"game\" + 0.006*\"live\" + 0.004*\"jimmi\"'), (1, '0.012*\"2018\" + 0.008*\"dizisi\" + 0.008*\"highlight\" + 0.008*\"fifa\" + 0.007*\"youtub\" + 0.006*\"atv\" + 0.005*\"رمضان\" + 0.005*\"entertain\" + 0.005*\"18\" + 0.005*\"simon\"'), (2, '0.023*\"show\" + 0.014*\"tv\" + 0.013*\"izl\" + 0.008*\"top\" + 0.007*\"dizi\" + 0.007*\"berlin\" + 0.007*\"10\" + 0.007*\"sport\" + 0.005*\"güldür\" + 0.005*\"fail\"')]\n",
      "-----------------------------------------------------\n",
      "[(0, '0.013*\"tv\" + 0.011*\"zap\" + 0.009*\"hallyday\" + 0.006*\"johnni\" + 0.006*\"la\" + 0.006*\"youtub\" + 0.005*\"nouvel\" + 0.005*\"et\" + 0.005*\"entertain\" + 0.005*\"test\"'), (1, '0.016*\"franc\" + 0.012*\"le\" + 0.007*\"top\" + 0.007*\"live\" + 0.006*\"télévision\" + 0.006*\"play\" + 0.006*\"10\" + 0.006*\"rap\" + 0.004*\"5\" + 0.004*\"2\"'), (2, '0.011*\"show\" + 0.009*\"tv\" + 0.008*\"2018\" + 0.008*\"foot\" + 0.007*\"late\" + 0.007*\"رمضان\" + 0.007*\"highlight\" + 0.006*\"pa\" + 0.006*\"comedi\" + 0.006*\"record\"')]\n",
      "-----------------------------------------------------\n",
      "[(0, '0.015*\"star\" + 0.012*\"war\" + 0.011*\"show\" + 0.010*\"cat\" + 0.009*\"late\" + 0.007*\"hip\" + 0.007*\"minaj\" + 0.007*\"danc\" + 0.007*\"2018\" + 0.007*\"new\"'), (1, '0.016*\"trailer\" + 0.013*\"movi\" + 0.008*\"live\" + 0.008*\"smith\" + 0.007*\"night\" + 0.007*\"drake\" + 0.007*\"song\" + 0.007*\"show\" + 0.006*\"anim\" + 0.006*\"venom\"'), (2, '0.017*\"music\" + 0.014*\"video\" + 0.011*\"record\" + 0.007*\"love\" + 0.007*\"talk\" + 0.005*\"sport\" + 0.005*\"youtub\" + 0.005*\"kid\" + 0.005*\"pop\" + 0.005*\"film\"')]\n",
      "-----------------------------------------------------\n",
      "[(0, '0.073*\"news\" + 0.032*\"video\" + 0.026*\"telugu\" + 0.022*\"tamil\" + 0.022*\"movi\" + 0.022*\"latest\" + 0.019*\"comedi\" + 0.012*\"tv9\" + 0.012*\"etv\" + 0.010*\"marathi\"'), (1, '0.033*\"tv\" + 0.031*\"song\" + 0.026*\"live\" + 0.020*\"show\" + 0.014*\"malayalam\" + 0.013*\"serial\" + 0.008*\"tamil\" + 0.007*\"gossip\" + 0.006*\"sharma\" + 0.006*\"love\"'), (2, '0.018*\"film\" + 0.012*\"india\" + 0.011*\"2018\" + 0.011*\"cinema\" + 0.010*\"comedi\" + 0.010*\"zee\" + 0.010*\"vine\" + 0.008*\"offici\" + 0.008*\"channel\" + 0.007*\"media\"')]\n"
     ]
    }
   ],
   "source": [
    "do_LDA_model(us_most_common_tags)\n",
    "print(\"-----------------------------------------------------\")\n",
    "do_LDA_model(ca_most_common_tags)\n",
    "print(\"-----------------------------------------------------\")\n",
    "do_LDA_model(de_most_common_tags)\n",
    "print(\"-----------------------------------------------------\")\n",
    "do_LDA_model(fr_most_common_tags)\n",
    "print(\"-----------------------------------------------------\")\n",
    "do_LDA_model(gb_most_common_tags)\n",
    "print(\"-----------------------------------------------------\")\n",
    "do_LDA_model(in_most_common_tags)\n",
    "\n",
    "### ----visualize maybe using pyLDAavis and maybe in a tabular format in a pandas df to show top topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "def do_search_youtube_request(videoId):\n",
    "    f = open(\"apiKey\", \"r\")\n",
    "    key = f.read()\n",
    "    url = \"https://www.googleapis.com/youtube/v3/search?part=snippet&maxResults=2&relatedToVideoId={}&type=video&key={}\".format(videoId, key)\n",
    "    r = requests.get(url)\n",
    "    return r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "def find_video_insights(videoIds):\n",
    "    f = open(\"apiKey\", \"r\")\n",
    "    key = f.read()\n",
    "    url = 'https://www.googleapis.com/youtube/v3/videos?part=snippet%2CcontentDetails%2Cstatistics&id=Ks-_Mh1QhMc%2Cc0KYU2j0TM4%2CeIho2S0ZahI&key={}'.format(key)\n",
    "    r = requests.get(url)\n",
    "    return r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P_If9xMyl10\n"
     ]
    }
   ],
   "source": [
    "## call this with 1 country at a time \n",
    "import json\n",
    "def process_youtube_requests(videoIds):\n",
    "    relatedVideoIds = []\n",
    "    for videoId in videoIds:\n",
    "        response = do_search_youtube_request(videoId)\n",
    "        relatedVideoId = response['items'][0]['id']['videoId']\n",
    "        relatedVideoIds += relatedVideoId\n",
    "    videoIdsStr = '%2C'.join([str(elem) for elem in relatedVideoIds])\n",
    "    r = find_video_insights(videoIdsStr)\n",
    "    parse_and_visualize(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7826\n"
     ]
    }
   ],
   "source": [
    "def parse_and_visualize(r):\n",
    "    tags = (r['items'][2]['snippet']['tags'])\n",
    "    channelTitles = (r['items'][2]['snippet']['channelTitle'])\n",
    "    descriptions = (r['items'][2]['snippet']['description'])\n",
    "    views = (r['items'][0]['statistics']['viewCount'])\n",
    "    likes = (r['items'][0]['statistics']['likeCount'])\n",
    "    dislikes = (r['items'][0]['statistics']['dislikeCount'])\n",
    "    favorites = (r['items'][0]['statistics']['favoriteCount'])\n",
    "    comments = (r['items'][0]['statistics']['commentCount'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
