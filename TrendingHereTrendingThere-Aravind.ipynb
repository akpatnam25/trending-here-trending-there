{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/botocore/awsrequest.py:624: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  class HeadersDict(collections.MutableMapping):\n",
      "/usr/local/lib/python3.7/site-packages/gensim/corpora/dictionary.py:11: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Mapping, defaultdict\n",
      "/usr/local/lib/python3.7/site-packages/scipy/sparse/sparsetools.py:21: DeprecationWarning: `scipy.sparse.sparsetools` is deprecated!\n",
      "scipy.sparse.sparsetools is a private module for scipy.sparse, and should not be used.\n",
      "  _deprecated()\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.book import *\n",
    "from nltk.corpus import stopwords\n",
    "import nltk.classify.util\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "import random\n",
    "from nltk import classify\n",
    "from nltk import NaiveBayesClassifier\n",
    "from nltk.tokenize import word_tokenize\n",
    "import seaborn as sns\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "sns.set_context('notebook')\n",
    "import warnings\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "from nltk.corpus import stopwords\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### get all csv dataframes\n",
    "US_df = pd.read_csv('youtube-new/USvideos.csv') #USA\n",
    "CA_df = pd.read_csv('youtube-new/CAvideos.csv') #CANADA\n",
    "DE_df = pd.read_csv('youtube-new/DEvideos.csv') #GERMANY\n",
    "FR_df = pd.read_csv('youtube-new/FRvideos.csv') #FRANCE\n",
    "GB_df = pd.read_csv('youtube-new/GBvideos.csv') #GREAT BRITAIN\n",
    "IN_df = pd.read_csv('youtube-new/INvideos.csv') #INDIA\n",
    "\n",
    "# JP_df = pd.read_csv('youtube-new/JPvideos.csv', encoding=\"UTF-8\") #JAPAN\n",
    "\n",
    "# KR_df = pd.read_csv('youtube-new/KRvideos.csv') #SOUTH KOREA\n",
    "\n",
    "# MX_df = pd.read_csv('youtube-new/MXvideos.csv') #MEXICO\n",
    "\n",
    "# RU_df = pd.read_csv('youtube-new/RUvideos.csv') #RUSSIA\n",
    "\n",
    "\n",
    "full_df = pd.concat([US_df, CA_df, DE_df, FR_df, GB_df, IN_df])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category_id</th>\n",
       "      <th>views</th>\n",
       "      <th>likes</th>\n",
       "      <th>dislikes</th>\n",
       "      <th>comment_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>40949.000000</td>\n",
       "      <td>4.094900e+04</td>\n",
       "      <td>4.094900e+04</td>\n",
       "      <td>4.094900e+04</td>\n",
       "      <td>4.094900e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>19.972429</td>\n",
       "      <td>2.360785e+06</td>\n",
       "      <td>7.426670e+04</td>\n",
       "      <td>3.711401e+03</td>\n",
       "      <td>8.446804e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>7.568327</td>\n",
       "      <td>7.394114e+06</td>\n",
       "      <td>2.288853e+05</td>\n",
       "      <td>2.902971e+04</td>\n",
       "      <td>3.743049e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.490000e+02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>2.423290e+05</td>\n",
       "      <td>5.424000e+03</td>\n",
       "      <td>2.020000e+02</td>\n",
       "      <td>6.140000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>6.818610e+05</td>\n",
       "      <td>1.809100e+04</td>\n",
       "      <td>6.310000e+02</td>\n",
       "      <td>1.856000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>1.823157e+06</td>\n",
       "      <td>5.541700e+04</td>\n",
       "      <td>1.938000e+03</td>\n",
       "      <td>5.755000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>2.252119e+08</td>\n",
       "      <td>5.613827e+06</td>\n",
       "      <td>1.674420e+06</td>\n",
       "      <td>1.361580e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        category_id         views         likes      dislikes  comment_count\n",
       "count  40949.000000  4.094900e+04  4.094900e+04  4.094900e+04   4.094900e+04\n",
       "mean      19.972429  2.360785e+06  7.426670e+04  3.711401e+03   8.446804e+03\n",
       "std        7.568327  7.394114e+06  2.288853e+05  2.902971e+04   3.743049e+04\n",
       "min        1.000000  5.490000e+02  0.000000e+00  0.000000e+00   0.000000e+00\n",
       "25%       17.000000  2.423290e+05  5.424000e+03  2.020000e+02   6.140000e+02\n",
       "50%       24.000000  6.818610e+05  1.809100e+04  6.310000e+02   1.856000e+03\n",
       "75%       25.000000  1.823157e+06  5.541700e+04  1.938000e+03   5.755000e+03\n",
       "max       43.000000  2.252119e+08  5.613827e+06  1.674420e+06   1.361580e+06"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###get most common tags\n",
    "def get_most_common_tags(country_df):\n",
    "    tags = country_df['tags'].to_string(index=False, header=False)\n",
    "    split_tags = [i.replace('\"', '') for i in tags.split(\"|\")]\n",
    "    stop_words = stopwords.words('english')\n",
    "    filtered_tags = [word for word in split_tags if word not in stop_words]\n",
    "    fdist = FreqDist(split_tags)\n",
    "    most_popular_tags = fdist.most_common(1000)\n",
    "    return fdist, dict(most_popular_tags)\n",
    "\n",
    "fd1, us_most_common_tags = get_most_common_tags(US_df)\n",
    "f2, ca_most_common_tags = get_most_common_tags(CA_df)\n",
    "fd3, de_most_common_tags = get_most_common_tags(DE_df)\n",
    "fd4, fr_most_common_tags = get_most_common_tags(FR_df)\n",
    "fd5, gb_most_common_tags = get_most_common_tags(GB_df)\n",
    "fd6, in_most_common_tags = get_most_common_tags(IN_df)\n",
    "\n",
    "US_df.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nword cloud \\nbar graph \\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####do visualization for most popular tags\n",
    "'''\n",
    "word cloud \n",
    "bar graph \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## do sentiment analysis on each of the tags\n",
    "positive_words_df = pd.read_fwf('positivewords.txt')\n",
    "negative_words_df = pd.read_fwf('negativewords.txt')\n",
    "\n",
    "def extract_features(words):\n",
    "    return dict([(word, True) for word in words.split()])\n",
    "\n",
    "def build_sentiment_analysis_model():\n",
    "    positive_words = positive_words_df['positivewords'].values.tolist()\n",
    "    negative_words = negative_words_df['negativewords'].values.tolist()\n",
    "    pos_feats = [(extract_features(f), 'positive') for f in positive_words ]\n",
    "    neg_feats = [(extract_features(f), 'negative') for f in negative_words ]\n",
    "    dataset = pos_feats + neg_feats\n",
    "    random.shuffle(dataset)\n",
    "    cutoff = int(0.80 * len(dataset))\n",
    "    train_data = dataset[:cutoff]\n",
    "    test_data = dataset[cutoff:]\n",
    "\n",
    "    classifier = NaiveBayesClassifier.train(train_data)\n",
    "    # print(\"Accuracy is:\", classify.accuracy(classifier, test_data))\n",
    "    # print(classifier.show_most_informative_features(10))\n",
    "    return classifier\n",
    "\n",
    "def execute_model(tags):\n",
    "    classifications = {}\n",
    "    classifier = build_sentiment_analysis_model()\n",
    "    classifier.show_most_informative_features(5)\n",
    "    for tag in tags:\n",
    "        classified = classifier.classify(extract_features(tag))\n",
    "        classifications[tag] = classified\n",
    "    return classifications\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "             enviousness = True           positi : negati =      2.4 : 1.0\n",
      "                 coolest = None           negati : positi =      1.0 : 1.0\n",
      "               righteous = None           negati : positi =      1.0 : 1.0\n",
      "                  snazzy = None           negati : positi =      1.0 : 1.0\n",
      "                 happily = None           negati : positi =      1.0 : 1.0\n",
      "Most Informative Features\n",
      "                 coolest = None           negati : positi =      1.0 : 1.0\n",
      "               righteous = None           negati : positi =      1.0 : 1.0\n",
      "                  snazzy = None           negati : positi =      1.0 : 1.0\n",
      "                 happily = None           negati : positi =      1.0 : 1.0\n",
      "                  firmer = None           negati : positi =      1.0 : 1.0\n",
      "Most Informative Features\n",
      "             enviousness = True           positi : negati =      2.4 : 1.0\n",
      "                 coolest = None           negati : positi =      1.0 : 1.0\n",
      "               righteous = None           negati : positi =      1.0 : 1.0\n",
      "                  snazzy = None           negati : positi =      1.0 : 1.0\n",
      "                 happily = None           negati : positi =      1.0 : 1.0\n",
      "Most Informative Features\n",
      "                 envious = True           positi : negati =      2.4 : 1.0\n",
      "             enviousness = True           positi : negati =      2.4 : 1.0\n",
      "               enviously = True           positi : negati =      2.4 : 1.0\n",
      "                 coolest = None           negati : positi =      1.0 : 1.0\n",
      "               righteous = None           negati : positi =      1.0 : 1.0\n",
      "Most Informative Features\n",
      "                 envious = True           positi : negati =      2.4 : 1.0\n",
      "               enviously = True           positi : negati =      2.4 : 1.0\n",
      "               righteous = None           negati : positi =      1.0 : 1.0\n",
      "                  snazzy = None           negati : positi =      1.0 : 1.0\n",
      "                 happily = None           negati : positi =      1.0 : 1.0\n",
      "Most Informative Features\n",
      "               enviously = True           positi : negati =      2.4 : 1.0\n",
      "                 coolest = None           negati : positi =      1.0 : 1.0\n",
      "               righteous = None           negati : positi =      1.0 : 1.0\n",
      "                  snazzy = None           negati : positi =      1.0 : 1.0\n",
      "                 happily = None           negati : positi =      1.0 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifications_us = execute_model(list(us_most_common_tags.keys()))\n",
    "classifications_ca = execute_model(list(ca_most_common_tags.keys()))\n",
    "classifications_de = execute_model(list(de_most_common_tags.keys()))\n",
    "classifications_fr = execute_model(list(fr_most_common_tags.keys()))\n",
    "classifications_gb = execute_model(list(gb_most_common_tags.keys()))\n",
    "classifications_in = execute_model(list(in_most_common_tags.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5% of the tags found in the trending youtube videos for country USA were positive in nature, while 97.5% were negative in nature.\n",
      "\n",
      "3.1% of the tags found in the trending youtube videos for country Canada were positive in nature, while 96.89999999999999% were negative in nature.\n",
      "\n",
      "1.0999999999999999% of the tags found in the trending youtube videos for country Denmakr were positive in nature, while 98.9% were negative in nature.\n",
      "\n",
      "1.3% of the tags found in the trending youtube videos for country France were positive in nature, while 98.7% were negative in nature.\n",
      "\n",
      "1.3% of the tags found in the trending youtube videos for country Great Britain were positive in nature, while 98.7% were negative in nature.\n",
      "\n",
      "1.9% of the tags found in the trending youtube videos for country India were positive in nature, while 98.1% were negative in nature.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_sentiment_stats(classification, country):\n",
    "    sentiments = list(classification.values())\n",
    "    sentiments_df = pd.DataFrame(sentiments, columns=['Sentiment'])\n",
    "    negatives = len(sentiments_df[sentiments_df['Sentiment'] =='negative'])\n",
    "    positives = len(sentiments_df[sentiments_df['Sentiment'] =='positive'])\n",
    "    total_len = len(sentiments_df)\n",
    "    percentage_of_negative = negatives / total_len * 100\n",
    "    percentage_of_positive = positives / total_len * 100\n",
    "    print (\"{}% of the tags found in the trending youtube videos for country {} were positive in nature, while {}% were negative in nature.\\n\".format(percentage_of_positive, country, percentage_of_negative))\n",
    "    \n",
    "get_sentiment_stats(classifications_us, \"USA\")\n",
    "get_sentiment_stats(classifications_ca, \"Canada\")\n",
    "get_sentiment_stats(classifications_de, \"Denmark\")\n",
    "get_sentiment_stats(classifications_fr, \"France\")\n",
    "get_sentiment_stats(classifications_gb, \"Great Britain\")\n",
    "get_sentiment_stats(classifications_in, \"India\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## do visualization of sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_LDA_model(country_tags):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "    # create English stop words list\n",
    "    stop_words = stopwords.words('english')\n",
    "\n",
    "    # Create p_stemmer of class PorterStemmer\n",
    "    p_stemmer = PorterStemmer()\n",
    "    # compile sample documents into a list\n",
    "    tags = list(country_tags.keys())\n",
    "\n",
    "    # list for tokenized documents in loop\n",
    "    texts = []\n",
    "\n",
    "    # loop through document list\n",
    "    for i in tags:\n",
    "\n",
    "        # clean and tokenize document string\n",
    "        raw = i.lower()\n",
    "        tokens = tokenizer.tokenize(raw)\n",
    "\n",
    "        # remove stop words from tokens\n",
    "        stopped_tokens = [i for i in tokens if not i in stop_words]\n",
    "\n",
    "        # stem tokens\n",
    "        stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "\n",
    "        # add tokens to list\n",
    "        texts.append(stemmed_tokens)\n",
    "\n",
    "    # turn our tokenized documents into a id <-> term dictionary\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "    # convert tokenized documents into a document-term matrix\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "    # generate LDA model\n",
    "    ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=3, id2word = dictionary, passes=100)\n",
    "    print(ldamodel.print_topics(num_topics=3, num_words=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.017*\"news\" + 0.010*\"video\" + 0.010*\"makeup\" + 0.009*\"music\" + 0.007*\"scienc\" + 0.007*\"beauti\" + 0.006*\"last\" + 0.006*\"iphon\" + 0.006*\"tutori\" + 0.006*\"pop\"'), (1, '0.012*\"anim\" + 0.007*\"sport\" + 0.007*\"film\" + 0.007*\"entertain\" + 0.006*\"one\" + 0.006*\"super\" + 0.006*\"puth\" + 0.006*\"diy\" + 0.006*\"review\" + 0.006*\"lip\"'), (2, '0.017*\"show\" + 0.012*\"movi\" + 0.011*\"new\" + 0.010*\"buzzfe\" + 0.010*\"cat\" + 0.009*\"live\" + 0.009*\"game\" + 0.008*\"late\" + 0.008*\"youtub\" + 0.008*\"video\"')]\n",
      "-----------------------------------------------------\n",
      "[(0, '0.025*\"news\" + 0.007*\"buzzfe\" + 0.007*\"ab\" + 0.006*\"cbn\" + 0.006*\"onlin\" + 0.006*\"jame\" + 0.006*\"laugh\" + 0.006*\"fail\" + 0.006*\"dr\" + 0.006*\"footbal\"'), (1, '0.017*\"tv\" + 0.017*\"show\" + 0.013*\"live\" + 0.012*\"nba\" + 0.011*\"highlight\" + 0.009*\"late\" + 0.008*\"trump\" + 0.008*\"anim\" + 0.007*\"world\" + 0.007*\"react\"'), (2, '0.012*\"video\" + 0.010*\"movi\" + 0.008*\"game\" + 0.008*\"fox\" + 0.008*\"2018\" + 0.007*\"sport\" + 0.007*\"funni\" + 0.007*\"youtub\" + 0.006*\"رمضان\" + 0.006*\"ted\"')]\n",
      "-----------------------------------------------------\n",
      "[(0, '0.011*\"deutsch\" + 0.010*\"die\" + 0.010*\"late\" + 0.007*\"music\" + 0.006*\"hop\" + 0.006*\"hip\" + 0.006*\"funni\" + 0.006*\"game\" + 0.006*\"live\" + 0.004*\"jimmi\"'), (1, '0.012*\"2018\" + 0.008*\"dizisi\" + 0.008*\"highlight\" + 0.008*\"fifa\" + 0.007*\"youtub\" + 0.006*\"atv\" + 0.005*\"رمضان\" + 0.005*\"entertain\" + 0.005*\"18\" + 0.005*\"simon\"'), (2, '0.023*\"show\" + 0.014*\"tv\" + 0.013*\"izl\" + 0.008*\"top\" + 0.007*\"dizi\" + 0.007*\"berlin\" + 0.007*\"10\" + 0.007*\"sport\" + 0.005*\"güldür\" + 0.005*\"fail\"')]\n",
      "-----------------------------------------------------\n",
      "[(0, '0.013*\"tv\" + 0.011*\"zap\" + 0.009*\"hallyday\" + 0.006*\"johnni\" + 0.006*\"la\" + 0.006*\"youtub\" + 0.005*\"nouvel\" + 0.005*\"et\" + 0.005*\"entertain\" + 0.005*\"test\"'), (1, '0.016*\"franc\" + 0.012*\"le\" + 0.007*\"top\" + 0.007*\"live\" + 0.006*\"télévision\" + 0.006*\"play\" + 0.006*\"10\" + 0.006*\"rap\" + 0.004*\"5\" + 0.004*\"2\"'), (2, '0.011*\"show\" + 0.009*\"tv\" + 0.008*\"2018\" + 0.008*\"foot\" + 0.007*\"late\" + 0.007*\"رمضان\" + 0.007*\"highlight\" + 0.006*\"pa\" + 0.006*\"comedi\" + 0.006*\"record\"')]\n",
      "-----------------------------------------------------\n",
      "[(0, '0.015*\"star\" + 0.012*\"war\" + 0.011*\"show\" + 0.010*\"cat\" + 0.009*\"late\" + 0.007*\"hip\" + 0.007*\"minaj\" + 0.007*\"danc\" + 0.007*\"2018\" + 0.007*\"new\"'), (1, '0.016*\"trailer\" + 0.013*\"movi\" + 0.008*\"live\" + 0.008*\"smith\" + 0.007*\"night\" + 0.007*\"drake\" + 0.007*\"song\" + 0.007*\"show\" + 0.006*\"anim\" + 0.006*\"venom\"'), (2, '0.017*\"music\" + 0.014*\"video\" + 0.011*\"record\" + 0.007*\"love\" + 0.007*\"talk\" + 0.005*\"sport\" + 0.005*\"youtub\" + 0.005*\"kid\" + 0.005*\"pop\" + 0.005*\"film\"')]\n",
      "-----------------------------------------------------\n",
      "[(0, '0.073*\"news\" + 0.032*\"video\" + 0.026*\"telugu\" + 0.022*\"tamil\" + 0.022*\"movi\" + 0.022*\"latest\" + 0.019*\"comedi\" + 0.012*\"tv9\" + 0.012*\"etv\" + 0.010*\"marathi\"'), (1, '0.033*\"tv\" + 0.031*\"song\" + 0.026*\"live\" + 0.020*\"show\" + 0.014*\"malayalam\" + 0.013*\"serial\" + 0.008*\"tamil\" + 0.007*\"gossip\" + 0.006*\"sharma\" + 0.006*\"love\"'), (2, '0.018*\"film\" + 0.012*\"india\" + 0.011*\"2018\" + 0.011*\"cinema\" + 0.010*\"comedi\" + 0.010*\"zee\" + 0.010*\"vine\" + 0.008*\"offici\" + 0.008*\"channel\" + 0.007*\"media\"')]\n"
     ]
    }
   ],
   "source": [
    "do_LDA_model(us_most_common_tags)\n",
    "print(\"-----------------------------------------------------\")\n",
    "do_LDA_model(ca_most_common_tags)\n",
    "print(\"-----------------------------------------------------\")\n",
    "do_LDA_model(de_most_common_tags)\n",
    "print(\"-----------------------------------------------------\")\n",
    "do_LDA_model(fr_most_common_tags)\n",
    "print(\"-----------------------------------------------------\")\n",
    "do_LDA_model(gb_most_common_tags)\n",
    "print(\"-----------------------------------------------------\")\n",
    "do_LDA_model(in_most_common_tags)\n",
    "\n",
    "### ----visualize maybe using pyLDAavis and maybe in a tabular format in a pandas df to show top topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "def do_search_youtube_request(videoId):\n",
    "    f = open(\"apiKey\", \"r\")\n",
    "    key = f.read()\n",
    "    url = \"https://www.googleapis.com/youtube/v3/search?part=snippet&maxResults=2&relatedToVideoId={}&type=video&key={}\".format(videoId, key)\n",
    "    r = requests.get(url)\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "def find_video_insights(videoIds):\n",
    "    f = open(\"apiKey\", \"r\")\n",
    "    key = f.read()\n",
    "    url = 'https://www.googleapis.com/youtube/v3/videos?part=snippet%2CcontentDetails%2Cstatistics&id=Ks-_Mh1QhMc%2Cc0KYU2j0TM4%2CeIho2S0ZahI&key={}'.format(key)\n",
    "    r = requests.get(url)\n",
    "    return r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P_If9xMyl10\n"
     ]
    }
   ],
   "source": [
    "## call this with 1 country at a time \n",
    "import json\n",
    "def process_youtube_requests(videoIds):\n",
    "    relatedVideoIds = []\n",
    "    for videoId in videoIds:\n",
    "        response = do_search_youtube_request(videoId)\n",
    "        relatedVideoId = response['items'][0]['id']['videoId']\n",
    "        relatedVideoIds += relatedVideoId\n",
    "    videoIdsStr = '%2C'.join([str(elem) for elem in relatedVideoIds])\n",
    "    r = find_video_insights(videoIdsStr)\n",
    "    parse_and_visualize(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      " \"kind\": \"youtube#searchListResponse\",\n",
      " \"etag\": \"\\\"j6xRRd8dTPVVptg711_CSPADRfg/rQkCC5VaxPCrPQVa4a4KOHdzRdU\\\"\",\n",
      " \"nextPageToken\": \"CAIQAA\",\n",
      " \"regionCode\": \"US\",\n",
      " \"pageInfo\": {\n",
      "  \"totalResults\": 107,\n",
      "  \"resultsPerPage\": 2\n",
      " },\n",
      " \"items\": [\n",
      "  {\n",
      "   \"kind\": \"youtube#searchResult\",\n",
      "   \"etag\": \"\\\"j6xRRd8dTPVVptg711_CSPADRfg/CHu3UAkJONZXY7pGAOVnz68WdTI\\\"\",\n",
      "   \"id\": {\n",
      "    \"kind\": \"youtube#video\",\n",
      "    \"videoId\": \"8JpFWNKWDuo\"\n",
      "   },\n",
      "   \"snippet\": {\n",
      "    \"publishedAt\": \"2016-03-26T06:35:14.000Z\",\n",
      "    \"channelId\": \"UCSAUGyc_xA8uYzaIVG6MESQ\",\n",
      "    \"title\": \"I Dare You! (ft. King Bach & DeStorm)\",\n",
      "    \"description\": \"Leave your dares in the comments section OR upvote the dares that you want to see in the next I DARE YOU video!\\n\\nCheck out Bach's channel!\\nhttp://www.youtube.com/BachelorsPadTv\\n\\nCheck out DeStorm's channel!\\nhttp://www.youtube.com/destorm\\n\\nJust Launched Official Store\\nhttps://www.gianthugs.com/ryan\\n\\nNigahiga Channel\\nhttp://www.youtube.com/nigahiga\\n\\nHigaTV Channel\\nhttp://www.youtube.com/higatv\\n\\nTwitter\\nhttp://www.twitter.com/therealryanhiga\\n\\nFacebook\\nhttp://www.facebook.com/higatv\\n\\nWebsite\\nhttp://www.higatv.com\\n\\nInstagram\\nhttp://www.instagram.com/notryanhiga\\n\\nSend us mail or whatever you want here!\\nPO Box 232355\\nLas Vegas, NV 89105\\n\\nDownload the TEEHEE app for iPhone and Android here:\\niPhone:http://goo.gl/KXLz9j  \\nAndroid: http://goo.gl/RQZUKR\",\n",
      "    \"thumbnails\": {\n",
      "     \"default\": {\n",
      "      \"url\": \"https://i.ytimg.com/vi/8JpFWNKWDuo/default.jpg\",\n",
      "      \"width\": 120,\n",
      "      \"height\": 90\n",
      "     },\n",
      "     \"medium\": {\n",
      "      \"url\": \"https://i.ytimg.com/vi/8JpFWNKWDuo/mqdefault.jpg\",\n",
      "      \"width\": 320,\n",
      "      \"height\": 180\n",
      "     },\n",
      "     \"high\": {\n",
      "      \"url\": \"https://i.ytimg.com/vi/8JpFWNKWDuo/hqdefault.jpg\",\n",
      "      \"width\": 480,\n",
      "      \"height\": 360\n",
      "     }\n",
      "    },\n",
      "    \"channelTitle\": \"nigahiga\",\n",
      "    \"liveBroadcastContent\": \"none\"\n",
      "   }\n",
      "  },\n",
      "  {\n",
      "   \"kind\": \"youtube#searchResult\",\n",
      "   \"etag\": \"\\\"j6xRRd8dTPVVptg711_CSPADRfg/VOJ0xucdhBUK9maJ_AbopBsZ5ts\\\"\",\n",
      "   \"id\": {\n",
      "    \"kind\": \"youtube#video\",\n",
      "    \"videoId\": \"ejqutWv-AVU\"\n",
      "   },\n",
      "   \"snippet\": {\n",
      "    \"publishedAt\": \"2017-10-06T18:38:15.000Z\",\n",
      "    \"channelId\": \"UCqNe2Mqtv5sNidn5hAAEfiQ\",\n",
      "    \"title\": \"RHPC Plays \\\"Fakin' it\\\"!\",\n",
      "    \"description\": \"Recently found this lost video from months ago!  Let us know if there's any other group games you wanna see us try out!\\n\\nOrder my book \\\"how to write good\\\"\\nhttp://higatv.com/ryan-higas-how-to-write-good-pre-order-links/\\n\\nNew Official Store\\nhttps://www.gianthugs.com/collections/ryan\\n\\nNigahiga Channel\\nhttp://www.youtube.com/nigahiga\\n\\nTwitter\\nhttp://www.twitter.com/therealryanhiga\\n\\nFacebook\\nhttp://www.facebook.com/higatv\\n\\nWebsite\\nhttp://www.higatv.com\\n\\nInstagram\\nhttp://www.instagram.com/notryanhiga\\n\\nSend us mail or whatever you want here!\\nPO Box 232355\\nLas Vegas, NV 89105\",\n",
      "    \"thumbnails\": {\n",
      "     \"default\": {\n",
      "      \"url\": \"https://i.ytimg.com/vi/ejqutWv-AVU/default.jpg\",\n",
      "      \"width\": 120,\n",
      "      \"height\": 90\n",
      "     },\n",
      "     \"medium\": {\n",
      "      \"url\": \"https://i.ytimg.com/vi/ejqutWv-AVU/mqdefault.jpg\",\n",
      "      \"width\": 320,\n",
      "      \"height\": 180\n",
      "     },\n",
      "     \"high\": {\n",
      "      \"url\": \"https://i.ytimg.com/vi/ejqutWv-AVU/hqdefault.jpg\",\n",
      "      \"width\": 480,\n",
      "      \"height\": 360\n",
      "     }\n",
      "    },\n",
      "    \"channelTitle\": \"HigaTV\",\n",
      "    \"liveBroadcastContent\": \"none\"\n",
      "   }\n",
      "  }\n",
      " ]\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = do_search_youtube_request(\"d380meD0W0M\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amy Cuddy|TED|TEDTalk|TEDTalks|TED Talk|TED Talks|TEDGlobal|brain|business|psychology|self|success\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User_ID</th>\n",
       "      <th>UserName</th>\n",
       "      <th>Action</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>Riti</td>\n",
       "      <td>Amy Cuddy|TED|TEDTalk|TEDTalks|TED Talk|TED Ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>Aadi</td>\n",
       "      <td>Logout</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  User_ID UserName                                             Action\n",
       "0      23     Riti  Amy Cuddy|TED|TEDTalk|TEDTalks|TED Talk|TED Ta...\n",
       "1      24     Aadi                                             Logout"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "id = 1\n",
    "tags = '|'.join((r['items'][0]['snippet']['tags']))\n",
    "title = (r['items'][0]['snippet']['title'])\n",
    "categoryId = (r['items'][0]['snippet']['categoryId'])\n",
    "channelTitle = (r['items'][0]['snippet']['channelTitle'])\n",
    "publishedAt = (r['items'][0]['snippet']['publishedAt'])\n",
    "description = (r['items'][0]['snippet']['description'])\n",
    "views = (r['items'][0]['statistics']['viewCount'])\n",
    "likes = (r['items'][0]['statistics']['likeCount'])\n",
    "dislikes = (r['items'][0]['statistics']['dislikeCount'])\n",
    "favorites = (r['items'][0]['statistics']['favoriteCount'])\n",
    "comments = (r['items'][0]['statistics']['commentCount'])\n",
    "print(tags)\n",
    "\n",
    "dfObj = pd.DataFrame(columns=['User_ID', 'UserName', 'Action'])\n",
    "dfObj = dfObj.append({'User_ID': 23, 'UserName': 'Riti', 'Action': tags}, ignore_index=True)\n",
    "dfObj = dfObj.append({'User_ID': 24, 'UserName': 'Aadi', 'Action': 'Logout'}, ignore_index=True)\n",
    " \n",
    "dfObj"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
